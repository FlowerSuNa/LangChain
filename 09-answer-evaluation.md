# RAG 답변 성능 평가


- LangSmith, LangFuse 사용
- [Guardrails](https://github.com/guardrails-ai/guardrails?tab=readme-ov-file) 사용



### 휴리스틱 평가

- 특정 규칙에 기반한 결정론적 함수로 작동하며, 명확한 기준에 따라 판단을 수행함
- 주로 단순 검증에 활용되며, 챗봇 응답의 공백 여부, 생성된 코드의 컴파일 가능함 등을 확인함
- 평가 기준이 명확하고 객관적이어서 정확한 분류나 검증이 필요한 경우에 효과적임
- 복잡한 상황보다는 명확한 규칙이 존재하는 간단한 검증 작업에 적합함

### 정량 평가 지표

- ROUGE와 BLEU는 텍스트 생성 품질을 평가하는 대표적인 정량 평가지표임
- 생성된 텍스트와 참조 텍스트 간의 단어 중첩도를 계산하여 품질을 수치화함
- 대규모 자동화 평가가 필요한 경우 효율적이며, 객관적인 비교가 가능한 장점이 있음
- 하지만, 문맥이나 의미의 유사성은 완벽하게 포착하지 못하는 한계점이 존재함
- 일반적인 챗봇에 쓰기는 모호하지만, RAG 기반이라면 쓸만하다고 생각됨

**ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**

- 생성된 요약문의 품질을 평가함

**BLEU (Billngual Evaluation Understudy)**

- 기계 번역의 품질을 평가하는 대표적인 지표로, 생성된 번역문과 참조 번역문 간의 n-gram 정확도를 계산함
- 0에서 1사이의 값을 가지며, 1에 가까울수록 번역 품질이 좋음을 의미함

- BP(Brevity Penalty) : 생성문이 참조문보다 짧을 경우 패널티를 부함함
    - min(1, exp(1-참조문길이/생성문길이))
- 일반적으로 BLEU-1부터 BLEU-4까지의 기하평균을 사용함

### 문자열 및 임베딩 거리 평가


