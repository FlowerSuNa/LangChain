{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151d7883",
   "metadata": {},
   "source": [
    "## í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c754de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcabc05",
   "metadata": {},
   "source": [
    "# LangGraph ë§›ë³´ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3358ec2c",
   "metadata": {},
   "source": [
    "## 1. ê¸°ë³¸ ì˜ˆì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69be331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, Literal\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# 1. ìƒíƒœ ì •ì˜\n",
    "class MyState(TypedDict):\n",
    "    name: str\n",
    "    is_morning: bool\n",
    "\n",
    "# 2. ë…¸ë“œ í•¨ìˆ˜ ì •ì˜\n",
    "def greet_user(state: MyState) -> MyState:\n",
    "    print(f\"Hi, {state['name']}!\")\n",
    "    return state\n",
    "\n",
    "def say_good_morning(state: MyState) -> MyState:\n",
    "    print(\"Good morning!\")\n",
    "    return state\n",
    "\n",
    "def say_hello(state: MyState) -> MyState:\n",
    "    print(\"Hello!\")\n",
    "    return state\n",
    "\n",
    "# 3. ì¡°ê±´ í•¨ìˆ˜ ì •ì˜\n",
    "def is_morning(state: MyState) -> Literal[\"morning\", \"not_morning\"]:\n",
    "    return \"morning\" if state[\"is_morning\"] else \"not_morning\"\n",
    "\n",
    "# 4. ê·¸ë˜í”„ êµ¬ì„±\n",
    "builder = StateGraph(MyState)\n",
    "\n",
    "builder.add_node(\"greet_user\", greet_user)\n",
    "builder.add_node(\"say_good_morning\", say_good_morning)\n",
    "builder.add_node(\"say_hello\", say_hello)\n",
    "\n",
    "builder.add_edge(START, \"greet_user\")\n",
    "builder.add_conditional_edges(\n",
    "    \"greet_user\",\n",
    "    is_morning,\n",
    "    {\n",
    "        \"morning\": \"say_good_morning\",\n",
    "        \"not_morning\": \"say_hello\",\n",
    "    },\n",
    ")\n",
    "builder.add_edge(\"say_good_morning\", END)\n",
    "builder.add_edge(\"say_hello\", END)\n",
    "\n",
    "# 5. ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "graph = builder.compile()\n",
    "\n",
    "# 6. ê·¸ë˜í”„ ì‹œê°í™”\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3be98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "graph.invoke({\"name\": \"Bob\", \"is_morning\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8b7c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "for step in graph.stream({\"name\": \"Bob\", \"is_morning\": False}, stream_mode=\"values\"):\n",
    "    print(step)\n",
    "    print(\"---\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed79c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "for step in graph.stream({\"name\": \"Bob\", \"is_morning\": False}, stream_mode=\"updates\"):\n",
    "    print(step)\n",
    "    print(\"---\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7054c0eb",
   "metadata": {},
   "source": [
    "## 2. ê³ ê¸‰ ê¸°ëŠ¥ ì‚¬ìš© ì˜ˆì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7fc3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Command\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, AnyMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from typing import TypedDict, Annotated\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜\n",
    "SUMMARY_PROMPT = \"You are a helpful assistant that summarizes the following text.\"\n",
    "EVALUATE_PROMPT = \"You are a helpful assistant that evaluates the quality of a summary.\\n\" \\\n",
    "                  \"You must provide a quality score between 0.0 and 1.0, where 0.0 is the lowest quality and 1.0 is the highest quality.\"\n",
    "IMPROVE_PROMPT = \"You are a helpful assistant that enhances low-quality summaries generated by AI.\\n\" \\\n",
    "                 \"Your goal is to rewrite them to be clearer, more accurate, and more natural.\"\n",
    "\n",
    "# ì¶œë ¥ êµ¬ì¡°í™”\n",
    "class Summary(BaseModel):\n",
    "    summary: Annotated[str, Field(description=\"The summary of the text\")]\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    quality: Annotated[float, Field(description=\"The quality of the summary\", ge=0, le=1)]\n",
    "\n",
    "# ëª¨ë¸ ì •ì˜\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "# ìƒíƒœ ì •ì˜\n",
    "class SummaryState(TypedDict):\n",
    "    text: str\n",
    "    summary: str\n",
    "    quality: float\n",
    "    finalized: bool\n",
    "    iteration: int\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "# 1. ìš”ì•½ ë…¸ë“œ\n",
    "def summarize_text(state: SummaryState) -> Command:\n",
    "    messages = [\n",
    "        SystemMessage(content=SUMMARY_PROMPT),\n",
    "        HumanMessage(content=f\"Please summarize the following text: {state['text']}\\n\\nSummary:\")\n",
    "    ]\n",
    "    response = llm.with_structured_output(Summary).invoke(messages)\n",
    "\n",
    "    print(f\"[summarize_text] ìš”ì•½ ì™„ë£Œ\")\n",
    "    return Command(\n",
    "        goto=\"evaluate_summary\",\n",
    "        update={\n",
    "            \"summary\": response.summary, \n",
    "            \"iteration\": 0,\n",
    "            \"messages\": messages + [AIMessage(content=response.summary)]\n",
    "        }\n",
    "    )\n",
    "\n",
    "# 2. í’ˆì§ˆ í‰ê°€ ë…¸ë“œ\n",
    "def evaluate_summary(state: SummaryState) -> Command:\n",
    "    messages = [\n",
    "        SystemMessage(content=EVALUATE_PROMPT),\n",
    "        HumanMessage(content=f\"The text is: {state['text']}\\n\\nPlease evaluate the following summary: {state['summary']}\")\n",
    "    ]\n",
    "    response = llm.with_structured_output(Evaluation).invoke(messages)\n",
    "\n",
    "    print(f\"[evaluate_summary] í‰ê°€ ê²°ê³¼: {response.quality}\")\n",
    "\n",
    "    # í’ˆì§ˆì— ë”°ë¼ ë‹¤ìŒ ë…¸ë“œ ë¶„ê¸° ë° ìƒíƒœ ì—…ë°ì´íŠ¸\n",
    "    return Command(\n",
    "        goto=\"finalize_summary\" if response.quality > 0.8 or state[\"iteration\"] > 3 else \"improve_summary\",\n",
    "        update={\n",
    "            \"quality\": response.quality, \n",
    "            \"messages\": messages + [AIMessage(content=str(response.quality))]\n",
    "        }\n",
    "    )\n",
    "\n",
    "# 3. ê°œì„  ë…¸ë“œ\n",
    "def improve_summary(state: SummaryState) -> Command:\n",
    "    messages = [\n",
    "        SystemMessage(content=IMPROVE_PROMPT),\n",
    "        HumanMessage(content=f\"The text is: {state['text']}\\n\\n\"\n",
    "                             f\"Please enhance the following summary: {state['summary']}\\n\\nEnhanced Summary:\")\n",
    "    ]\n",
    "    response = llm.with_structured_output(Summary).invoke(messages)\n",
    "\n",
    "    print(f\"[improve_summary] ìš”ì•½ ìˆ˜ì •ë¨\")\n",
    "    return Command(\n",
    "        goto=\"evaluate_summary\",\n",
    "        update={\n",
    "            \"summary\": response.summary, \n",
    "            \"iteration\": state[\"iteration\"] + 1,\n",
    "            \"messages\": messages + [AIMessage(content=response.summary)]\n",
    "        }\n",
    "    )\n",
    "\n",
    "# 4. ìµœì¢…í™” ë…¸ë“œ\n",
    "def finalize_summary(state: SummaryState) -> Command:\n",
    "    print(f\"[finalize_summary] ìµœì¢… ìš”ì•½ ì™„ë£Œ\")\n",
    "    return Command(\n",
    "        goto=END,\n",
    "        update={\"finalized\": True}\n",
    "    )\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„±\n",
    "builder = StateGraph(SummaryState)\n",
    "\n",
    "builder.add_node(\"summarize_text\", summarize_text)\n",
    "builder.add_node(\"evaluate_summary\", evaluate_summary)\n",
    "builder.add_node(\"improve_summary\", improve_summary)\n",
    "builder.add_node(\"finalize_summary\", finalize_summary)\n",
    "\n",
    "builder.add_edge(START, \"summarize_text\")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤\n",
    "text = \"\"\"\n",
    "The app is useful. I use it every day. Some features are hard to find. But I still use it. Itâ€™s okay.\n",
    "\"\"\"\n",
    "for step in graph.stream({\"text\": text}, stream_mode=\"updates\"):\n",
    "    print(step)\n",
    "    print(\"---\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9532424",
   "metadata": {},
   "source": [
    "## 3. Send ì˜ˆì œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cd1374",
   "metadata": {},
   "source": [
    "> Map-Reduce íŒ¨í„´ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971f79f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "from langchain_core.documents import Document\n",
    "from typing import TypedDict, List, Annotated\n",
    "from operator import add\n",
    "from IPython.display import Image, display\n",
    "from pprint import pprint\n",
    "\n",
    "# ì „ì²´ ìƒíƒœ ì •ì˜\n",
    "class OverallState(TypedDict):\n",
    "    docs: List[Document]\n",
    "    summaries: Annotated[List[str], add]\n",
    "\n",
    "# ë¡œì»¬ ìƒíƒœ ì •ì˜\n",
    "class DocState(TypedDict):\n",
    "    doc: Document\n",
    "\n",
    "# ë¼ìš°í„° í•¨ìˆ˜\n",
    "def map_router(state: OverallState) -> List[Send]:\n",
    "    return [Send(\"map_node\", {\"doc\": doc}) for doc in state[\"docs\"]]\n",
    "\n",
    "# ë§µ ë…¸ë“œ í•¨ìˆ˜\n",
    "def map_node(state: DocState) -> OverallState:\n",
    "    doc = state[\"doc\"]\n",
    "    summary = f\"{doc.metadata['source']}: {doc.page_content[:30]}...\"\n",
    "    return {\"summaries\": [summary]}\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„±\n",
    "builder = StateGraph(OverallState)\n",
    "\n",
    "builder.add_node(\"map_node\", map_node)\n",
    "builder.add_conditional_edges(START, map_router, [\"map_node\"])\n",
    "builder.add_edge(\"map_node\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain is a framework for building LLM-powered apps.\", metadata={\"source\": \"intro\"}),\n",
    "    Document(page_content=\"StateGraph enables structured workflows with shared state.\", metadata={\"source\": \"graph\"}),\n",
    "    Document(page_content=\"Send can dynamically route data to other nodes.\", metadata={\"source\": \"send\"}),\n",
    "]\n",
    "result = graph.invoke({\"docs\": docs})\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1843e212",
   "metadata": {},
   "source": [
    "# ReAct Agent êµ¬í˜„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19c6082",
   "metadata": {},
   "source": [
    "## 1. Tool ë°”ì¸ë”© í™œìš©\n",
    "> `ToolNode`ì™€ `Tool` ë°”ì¸ë”©ì„ ì‚¬ìš©í•œ Reasoning + Acting íŒ¨í„´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7800fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.types import Send\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "# ê²€ìƒ‰ ë„êµ¬\n",
    "search_tool = DuckDuckGoSearchRun(name=\"search_tool\")\n",
    "\n",
    "# ë‚ ì”¨ ë„êµ¬\n",
    "@tool\n",
    "def weather_tool(latitude: float, longitude: float) -> str:\n",
    "    \"\"\"Get current weather information for a specific latitude/longitude.\n",
    "    \n",
    "    Args:\n",
    "        latitude: Latitude coordinate\n",
    "        longitude: Longitude coordinate\n",
    "        \n",
    "    Returns:\n",
    "        Weather information for the coordinates\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "        params = {\n",
    "            \"latitude\": latitude,\n",
    "            \"longitude\": longitude,\n",
    "            \"current_weather\": \"true\",\n",
    "            \"hourly\": \"relative_humidity_2m\"\n",
    "        }\n",
    "        resp = requests.get(url, params=params, timeout=5)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        # ê¸°ì˜¨\n",
    "        temp = data.get(\"current_weather\", {}).get(\"temperature\")\n",
    "        # ìŠµë„: ê°€ì¥ ê°€ê¹Œìš´ ì‹œê°„ì˜ ìŠµë„ ì‚¬ìš©\n",
    "        humidity = \"N/A\"\n",
    "        if \"hourly\" in data and \"relative_humidity_2m\" in data[\"hourly\"]:\n",
    "            # í˜„ì¬ ì‹œê°„ index ì°¾ê¸°\n",
    "            current_time = data[\"current_weather\"][\"time\"]\n",
    "            times = data[\"hourly\"][\"time\"]\n",
    "            humidities = data[\"hourly\"][\"relative_humidity_2m\"]\n",
    "            if current_time in times:\n",
    "                idx = times.index(current_time)\n",
    "                humidity = humidities[idx]\n",
    "            else:\n",
    "                humidity = humidities[0]\n",
    "        return f\"í˜„ì¬ ê¸°ì˜¨: {temp}Â°C, ìŠµë„: {humidity}%\"\n",
    "    except Exception as e:\n",
    "        return f\"ë‚ ì”¨ API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}\"\n",
    "\n",
    "# LLM ì •ì˜\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# ë„êµ¬ ì •ì˜\n",
    "tools = [search_tool, weather_tool]\n",
    "\n",
    "# LLMì— ë„êµ¬ ë°”ì¸ë”©\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì •ì˜\n",
    "system_message = SystemMessage(content=\"\"\"You are a helpful AI assistant that can use tools to answer questions.\n",
    "\n",
    "Available tools:\n",
    "- search_tool: For searching information  \n",
    "- weather_tool: For weather information\n",
    "\n",
    "When you need to use a tool, think step by step:\n",
    "1. Identify what information you need\n",
    "2. Choose the appropriate tool\n",
    "3. Use the tool with the correct parameters\n",
    "4. Analyze the results\n",
    "5. Provide a helpful response\n",
    "\n",
    "If you can answer directly without tools, do so. Always be helpful and accurate.\"\"\")\n",
    "\n",
    "# ìƒíƒœ ì •ì˜\n",
    "class AgentState(MessagesState):\n",
    "    ...\n",
    "\n",
    "# ì—ì´ì „íŠ¸ ë…¸ë“œ\n",
    "def agent_node(state: AgentState) -> List[Send]:\n",
    "    messages = [system_message] + state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    print(f\"[Agent] ì‘ë‹µ ìƒì„±: {response.content[:20]}...\")\n",
    "    if response.tool_calls:\n",
    "        print(f\"[Agent] ë„êµ¬ í˜¸ì¶œ: {[tool['name'] for tool in response.tool_calls]}\")\n",
    "\n",
    "        return [Send(\"tools\", {\n",
    "                \"tool_name\": tool_call[\"name\"], \n",
    "                \"parameters\": tool_call.get(\"parameters\", {})\n",
    "            }) for tool_call in response.tool_calls\n",
    "        ]\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# ê·¸ë˜í”„ êµ¬ì„±\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# ë…¸ë“œ ì¶”ê°€\n",
    "workflow.add_node(\"agent\", agent_node)      # ì—ì´ì „íŠ¸ ë…¸ë“œ\n",
    "workflow.add_node(\"tools\", ToolNode(tools)) # ë„êµ¬ ì‹¤í–‰ ë…¸ë“œ\n",
    "\n",
    "# ì—£ì§€ ì„¤ì •\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tools_condition,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "react_agent = workflow.compile()\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹œê°í™”\n",
    "display(Image(react_agent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0d25d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === í…ŒìŠ¤íŠ¸ ì˜ˆì œë“¤ ===\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ 1: ê³„ì‚° ì§ˆë¬¸\n",
    "def test_calculation():\n",
    "    print(\"=== í…ŒìŠ¤íŠ¸ 1: ê³„ì‚° ì§ˆë¬¸ ===\")\n",
    "    question = \"What is 25 * 4 + 18?\"\n",
    "    \n",
    "    result = react_agent.invoke({\n",
    "        \"messages\": [HumanMessage(content=question)]\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nì§ˆë¬¸: {question}\")\n",
    "    print(f\"ìµœì¢… ë‹µë³€: {result['messages'][-1].content}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ 2: ê²€ìƒ‰ ì§ˆë¬¸  \n",
    "def test_search():\n",
    "    print(\"=== í…ŒìŠ¤íŠ¸ 2: ê²€ìƒ‰ ì§ˆë¬¸ ===\")\n",
    "    question = \"What is Python programming language?\"\n",
    "    \n",
    "    result = react_agent.invoke({\n",
    "        \"messages\": [HumanMessage(content=question)]\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nì§ˆë¬¸: {question}\")\n",
    "    print(f\"ìµœì¢… ë‹µë³€: {result['messages'][-1].content}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ 3: ë‚ ì”¨ ì§ˆë¬¸\n",
    "def test_weather():\n",
    "    print(\"=== í…ŒìŠ¤íŠ¸ 3: ë‚ ì”¨ ì§ˆë¬¸ ===\") \n",
    "    question = \"What's the weather like in Seoul?\"\n",
    "    \n",
    "    result = react_agent.invoke({\n",
    "        \"messages\": [HumanMessage(content=question)]\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nì§ˆë¬¸: {question}\")\n",
    "    print(f\"ìµœì¢… ë‹µë³€: {result['messages'][-1].content}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ 4: ë³µí•© ì§ˆë¬¸ (ì—¬ëŸ¬ ë„êµ¬ ì‚¬ìš©)\n",
    "def test_complex():\n",
    "    print(\"=== í…ŒìŠ¤íŠ¸ 4: ë³µí•© ì§ˆë¬¸ ===\")\n",
    "    question = \"If I have 100 dollars and buy 3 items that cost 15 dollars each, how much money do I have left? Also, what's the weather in Tokyo?\"\n",
    "    \n",
    "    # stream ëª¨ë“œë¡œ ë‹¨ê³„ë³„ ê³¼ì • í™•ì¸\n",
    "    print(f\"ì§ˆë¬¸: {question}\\n\")\n",
    "    \n",
    "    for chunk in react_agent.stream(\n",
    "        {\"messages\": [HumanMessage(content=question)]},\n",
    "        stream_mode=\"updates\"\n",
    "    ):\n",
    "        node_name = list(chunk.keys())[0]\n",
    "        if node_name == \"agent\":\n",
    "            message = chunk[node_name][\"messages\"][0]\n",
    "            print(f\"ğŸ¤– Agent: {message.content[:100]}...\")\n",
    "            if hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "                for tool_call in message.tool_calls:\n",
    "                    print(f\"   ğŸ”§ ë„êµ¬ í˜¸ì¶œ: {tool_call['name']}\")\n",
    "        elif node_name == \"tools\":\n",
    "            tool_messages = chunk[node_name][\"messages\"]\n",
    "            for msg in tool_messages:\n",
    "                if hasattr(msg, 'content'):\n",
    "                    print(f\"   âš™ï¸  ë„êµ¬ ê²°ê³¼: {msg.content}\")\n",
    "        print()\n",
    "\n",
    "# ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "def run_all_tests():\n",
    "    test_calculation()\n",
    "    test_search()\n",
    "    test_weather() \n",
    "    test_complex()\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "run_all_tests()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c5cb0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
