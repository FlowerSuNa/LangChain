{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ì£¼íƒì²­ì•½ FAQ ì‹œìŠ¤í…œ ì±—ë´‡ êµ¬í˜„ \n",
    "\n",
    "- ë¬¸ì„œ ì „ì²˜ë¦¬ + RAG + Gradio ChatInterface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í™˜ê²½ ë³€ìˆ˜\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM ì„¤ì •\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model='gpt-4.1-mini',\n",
    "    temperature=0.1,\n",
    "    top_p=0.8, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë¬¸ì„œ ì „ì²˜ë¦¬\n",
    "\n",
    "* ë°ì´í„° ì •ì œ : ì›ë³¸ ë¬¸ì„œì—ì„œ HTML íƒœê·¸, íŠ¹ìˆ˜ë¬¸ì, ì¤‘ë³µ ë¬¸ì¥ ë“±ì„ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë¥¼ í‘œì¤€í™”í•˜ì—¬ ê²€ìƒ‰ í’ˆì§ˆì„ ë†’ì„\n",
    "* ë¬¸ì„œ ì²­í‚¹(Chunking) : ë¬¸ì„œë¥¼ ë¬¸ë§¥ì´ ìœ ì§€ë˜ë„ë¡ ë¬¸ì¥ ë˜ëŠ” ë‹¨ë½ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ì™€ ì‘ë‹µ í’ˆì§ˆì„ í–¥ìƒì‹œí‚´\n",
    "* ì„ë² ë”©(Embedding) : í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ê¸°ë°˜ ê³ ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ì´ ê°€ëŠ¥í•˜ë„ë¡ í•¨\n",
    "* ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒ‰ì¸í™” : ì„ë² ë”©ëœ ë²¡í„°ë¥¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ìƒ‰ì¸í™”í•˜ì—¬ ëŒ€ê·œëª¨ ë¬¸ì„œì—ì„œë„ ë¹ ë¥¸ ê²€ìƒ‰ì„ ê°€ëŠ¥í•˜ê²Œ í•¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) ë¬¸ì„œ ë¡œë“œ\n",
    "\n",
    "- êµ­í† êµí†µë¶€ ì£¼íƒì²­ì•½ FAQì—ì„œ ì¼ë¶€ ë‚´ìš©(ì²­ì•½ìê²©, ì²­ì•½í†µì¥)ì„ ë°œì·Œí•˜ì—¬ ì¬ê°€ê³µí•œ ë¬¸ì„œë¡œ, 50ê°œì˜ ë¬¸ë‹µì´ í¬í•¨ëœ í…ìŠ¤íŠ¸ íŒŒì¼ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\n",
    "    file_path=\"data/housing_faq.txt\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 ê²½ê¸°ë„ ê³¼ì²œì‹œì—ì„œ ê³µê¸‰ë˜ëŠ” ì£¼íƒì˜ í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì˜ ë²”ìœ„ëŠ”?\n",
      "A í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì´ë€ íŠ¹ë³„ì‹œã†ê´‘ì—­ì‹œã†íŠ¹ë³„ìì¹˜ì‹œã†íŠ¹ë³„ìì¹˜ë„(ê´€í•  êµ¬ì—­ ì•ˆì— ì§€ë°©ìì¹˜ë‹¨ì²´ì¸ ì‹œã†êµ°ì´ ì—†ëŠ” íŠ¹ë³„ìì¹˜ë„ë¥¼ ë§í•œë‹¤) ë˜ëŠ” ì‹œã†êµ°ì˜ í–‰ì •êµ¬ì—­ì„ ë§í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ê²½ê¸°ë„ ê³¼ì²œì‹œì—ì„œ ê³µê¸‰í•˜ëŠ” ì£¼íƒì˜ ê²½ìš° ê³¼ì²œì‹œê°€ í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì— í•´ë‹¹ë©ë‹ˆë‹¤. \n",
      "ì°¸ê³ ë¡œ, ì„œìš¸íŠ¹ë³„ì‹œì—ì„œ ê³µê¸‰ë˜ëŠ” ì£¼íƒì˜ ê²½ìš° ì„œìš¸íŠ¹ë³„ì‹œ ì „ì—­, ì¸ì²œê´‘ì—­ì‹œì˜ ê²½ìš° ì¸ì²œê´‘ì—­ì‹œ ì „ì—­ì´ í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì— í•´ë‹¹ë©ë‹ˆë‹¤.\n",
      "\n",
      "Q2 í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì— ê±°ì£¼í•˜ê³  ìˆì§€ ì•Šë‹¤ë©´ ì²­ì•½ì‹ ì²­ì´ ë¶ˆê°€ëŠ¥í•œì§€?\n",
      "A í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì— ê±°ì£¼í•˜ê³  ìˆì§€ ì•Šë”ë¼ë„ ì²­ì•½ê°€ëŠ¥ì§€ì—­ì—ì„œ ê³µê¸‰ë˜ëŠ” ì£¼íƒì— ì²­ì•½ì‹ ì²­ì´ ê°€ëŠ¥í•˜ë‚˜, ê°™ì€ ìˆœìœ„ì—ì„œëŠ” í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì˜ ê±°ì£¼ìê°€ ìš°ì„ í•˜ì—¬ ì£¼íƒì„ ê³µê¸‰ë°›ê²Œ ë©ë‹ˆë‹¤.\n",
      "* ì„œìš¸Â·ì¸ì²œÂ·ê²½ê¸°ë„ / ëŒ€ì „Â·ì„¸ì¢…Â·ì¶©ë‚¨ / ì¶©ë¶ / ê´‘ì£¼Â·ì „ë‚¨ / ì „ë¶ / ëŒ€êµ¬Â·ê²½ë¶ / ë¶€ì‚°Â·ìš¸ì‚°Â·ê²½ë‚¨ / ê°•ì›\n",
      "ë‹¤ë§Œ, ìˆ˜ë„ê¶Œ ëŒ€ê·œëª¨ íƒì§€ê°œë°œì§€êµ¬ ë“±ì—ì„œ ì£¼íƒì´ ê³µê¸‰ë˜ëŠ” ê²½ìš° ì¼ì • ë¹„ìœ¨ì˜ \n"
     ]
    }
   ],
   "source": [
    "# ë¬¸ì„œ í™•ì¸\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'data/housing_faq.txt'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¬¸ì„œ ë©”íƒ€ë°ì´í„° í™•ì¸\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) ë¬¸ì„œ ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) ì •ê·œí‘œí˜„ì‹ì„ í™œìš©í•˜ì—¬ ë¬¸ì„œì—ì„œ ì§ˆë¬¸-ë‹µë³€ ìŒì„ êµ¬ì¡°ì ìœ¼ë¡œ ë¶„ë¦¬í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_qa_pairs(text):\n",
    "    qa_pairs = []\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ë¥¼ ë¼ì¸ë³„ë¡œ ë¶„ë¦¬í•˜ê³  ê° ë¼ì¸ì˜ ì•ë’¤ ê³µë°± ì œê±°\n",
    "    lines = [line.strip() for line in text.split('\\n')]\n",
    "    current_question = None\n",
    "    current_answer = []\n",
    "    current_number = None\n",
    "    in_answer = False\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if not line:  # ë¹ˆ ë¼ì¸ ì²˜ë¦¬\n",
    "            if in_answer and current_answer and i + 1 < len(lines) and lines[i + 1].startswith('Q'):\n",
    "                # ë‹¤ìŒ ì§ˆë¬¸ì´ ì‹œì‘ë˜ê¸° ì „ ë¹ˆ ì¤„ì´ë©´ í˜„ì¬ QA ìŒ ì €ì¥\n",
    "                qa_pairs.append({\n",
    "                    'number': current_number,\n",
    "                    'question': current_question,\n",
    "                    'answer': ' '.join(current_answer).strip()\n",
    "                })\n",
    "                in_answer = False\n",
    "                current_answer = []\n",
    "            continue\n",
    "            \n",
    "        # ìƒˆë¡œìš´ ì§ˆë¬¸ í™•ì¸ (Q ë‹¤ìŒì— ìˆ«ìê°€ ì˜¤ëŠ” íŒ¨í„´)\n",
    "        q_match = re.match(r'Q(\\d+)\\s+(.*)', line)\n",
    "        if q_match:\n",
    "            # ì´ì „ QA ìŒì´ ìˆìœ¼ë©´ ì €ì¥\n",
    "            if current_question is not None and current_answer:\n",
    "                qa_pairs.append({\n",
    "                    'number': current_number,\n",
    "                    'question': current_question,\n",
    "                    'answer': ' '.join(current_answer).strip()\n",
    "                })\n",
    "            \n",
    "            # ìƒˆë¡œìš´ ì§ˆë¬¸ ì‹œì‘\n",
    "            current_number = int(q_match.group(1))\n",
    "            current_question = q_match.group(2).strip().rstrip('?') + '?'  # ì§ˆë¬¸ ë§ˆí¬ ì •ê·œí™”\n",
    "            current_answer = []\n",
    "            in_answer = False\n",
    "            \n",
    "        # ë‹µë³€ ì‹œì‘ í™•ì¸\n",
    "        elif line.startswith('A ') or (current_question and not current_answer and line):\n",
    "            in_answer = True\n",
    "            current_answer.append(line.lstrip('A '))\n",
    "            \n",
    "        # ê¸°ì¡´ ë‹µë³€ì— ë‚´ìš© ì¶”ê°€\n",
    "        elif current_question is not None and (in_answer or not line.startswith('Q')):\n",
    "            if in_answer or (current_answer and not line.startswith('Q')):\n",
    "                current_answer.append(line)\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ QA ìŒ ì²˜ë¦¬\n",
    "    if current_question is not None and current_answer:\n",
    "        qa_pairs.append({\n",
    "            'number': current_number,\n",
    "            'question': current_question,\n",
    "            'answer': ' '.join(current_answer).strip()\n",
    "        })\n",
    "    \n",
    "    # ë²ˆí˜¸ ìˆœì„œëŒ€ë¡œ ì •ë ¬\n",
    "    qa_pairs.sort(key=lambda x: x['number'])\n",
    "    \n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¶”ì¶œëœ QA ìŒ ê°œìˆ˜: 50\n",
      "ì¶”ì¶œëœ ì²«ë²ˆì§¸ QA: \n",
      "{'number': 1, 'question': 'ê²½ê¸°ë„ ê³¼ì²œì‹œì—ì„œ ê³µê¸‰ë˜ëŠ” ì£¼íƒì˜ í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì˜ ë²”ìœ„ëŠ”?', 'answer': 'í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì´ë€ íŠ¹ë³„ì‹œã†ê´‘ì—­ì‹œã†íŠ¹ë³„ìì¹˜ì‹œã†íŠ¹ë³„ìì¹˜ë„(ê´€í•  êµ¬ì—­ ì•ˆì— ì§€ë°©ìì¹˜ë‹¨ì²´ì¸ ì‹œã†êµ°ì´ ì—†ëŠ” íŠ¹ë³„ìì¹˜ë„ë¥¼ ë§í•œë‹¤) ë˜ëŠ” ì‹œã†êµ°ì˜ í–‰ì •êµ¬ì—­ì„ ë§í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ê²½ê¸°ë„ ê³¼ì²œì‹œì—ì„œ ê³µê¸‰í•˜ëŠ” ì£¼íƒì˜ ê²½ìš° ê³¼ì²œì‹œê°€ í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì— í•´ë‹¹ë©ë‹ˆë‹¤. ì°¸ê³ ë¡œ, ì„œìš¸íŠ¹ë³„ì‹œì—ì„œ ê³µê¸‰ë˜ëŠ” ì£¼íƒì˜ ê²½ìš° ì„œìš¸íŠ¹ë³„ì‹œ ì „ì—­, ì¸ì²œê´‘ì—­ì‹œì˜ ê²½ìš° ì¸ì²œê´‘ì—­ì‹œ ì „ì—­ì´ í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì— í•´ë‹¹ë©ë‹ˆë‹¤.'}\n"
     ]
    }
   ],
   "source": [
    "# QA ìŒ ì¶”ì¶œ\n",
    "qa_pairs = extract_qa_pairs(docs[0].page_content) \n",
    "\n",
    "print(f\"ì¶”ì¶œëœ QA ìŒ ê°œìˆ˜: {len(qa_pairs)}\")\n",
    "print(f\"ì¶”ì¶œëœ ì²«ë²ˆì§¸ QA: \\n{qa_pairs[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) LLMì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œì™€ í•µì‹¬ ê°œë…ì„ ì¶”ì¶œí•˜ê³ , ì´ë¥¼ ë©”íƒ€ë°ì´í„°ë‚˜ ë³¸ë¬¸ì— ì¶”ê°€í•˜ì—¬ ê²€ìƒ‰ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í‚¤ì›Œë“œ: ì…ì£¼ìì €ì¶•\n",
      "ìš”ì•½: ì œ2ìˆœìœ„ ì²­ì•½ì‹ ì²­ ì‹œ ì…ì£¼ìì €ì¶• í†µì¥ì´ í•„ìš”í•˜ë©°, ë¶„ì–‘ì£¼íƒ ë˜ëŠ” ë¶„ì–‘ì „í™˜ê³µê³µì„ëŒ€ì£¼íƒ ì…ì£¼ìë¡œ ì„ ì •ëœ ê²½ìš° í•´ë‹¹ í†µì¥ì€ ì¬ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# ì¶œë ¥ í˜•ì‹ ì •ì˜\n",
    "class KeywordOutput(BaseModel):\n",
    "    keyword: str = Field(description=\"ì§ˆë¬¸ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œí•œ ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œ\")\n",
    "    summary: str = Field(description=\"ì§ˆë¬¸ê³¼ ì‘ë‹µ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ë‚´ìš© ìš”ì•½\")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
    "template = \"\"\"ì£¼ì–´ì§„ ì§ˆë¬¸ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•˜ê³ , ì‘ë‹µ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•©ë‹ˆë‹¤.\n",
    "ì§ˆë¬¸ ë° ì‘ë‹µ í…ìŠ¤íŠ¸ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ í•µì‹¬ ìš©ì–´ë‚˜ ì „ë¬¸ ìš©ì–´, ì£¼ìš” ì•„ì´ë””ì–´ë‚˜ ì›ë¦¬ ë“±ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "\n",
    "ì§ˆë¬¸ í…ìŠ¤íŠ¸:\n",
    "{question}\n",
    "\n",
    "ì‘ë‹µ í…ìŠ¤íŠ¸:\n",
    "{answer}\n",
    "\n",
    "JSON í˜•ì‹ìœ¼ë¡œ ë‹¤ìŒ ì •ë³´ë¥¼ ë°˜í™˜í•˜ì‹œì˜¤:\n",
    "- keyword: ë‹¨ì–´ 1ê°œ ë§Œ\n",
    "- summary: 1-2 ë¬¸ì¥ ë§Œ\n",
    "\"\"\"\n",
    "\n",
    "# LCEL ì²´ì¸ êµ¬ì„± (Sturctured Output ì‚¬ìš©)\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm_with_structure = llm.with_structured_output(KeywordOutput)\n",
    "keyowrd_extractor = prompt | llm_with_structure\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸     \n",
    "result = keyowrd_extractor.invoke({\n",
    "    \"question\": qa_pairs[36]['question'],\n",
    "    \"answer\": qa_pairs[36]['answer']\n",
    "})\n",
    "print(\"í‚¤ì›Œë“œ:\", result.keyword)\n",
    "print(\"ìš”ì•½:\", result.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) ìš”ì•½ë¬¸ì„ ì‹œë§¨í‹± ê²€ìƒ‰ì— í™œìš©í•˜ê¸° ìœ„í•´, ìš”ì•½ì„ `page_content`ì— ë‹´ê³  ê¸°íƒ€ ì •ë³´ë¥¼ `metadata`ë¡œ êµ¬ì„±í•œ ë¬¸ì„œ ê°ì²´ë¥¼ ìƒì„±í•¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í¬ë§·íŒ…ëœ ë¬¸ì„œ ê°œìˆ˜: 50\n",
      "ëŒ€ê·œëª¨íƒì§€ê°œë°œì§€êµ¬ì—ì„œ ì£¼íƒ ê³µê¸‰ ì‹œ íŠ¹ë³„ê³µê¸‰ ë¬¼ëŸ‰ë„ ê³µê¸‰ ë¹„ìœ¨ì— ë”°ë¼ ë°°ì •ë˜ë©°, ê±°ì£¼ì§€ë³„ ìš°ì„ ê³µê¸‰ ë¹„ìœ¨ì´ ì ìš©ëœë‹¤. ë‹¤ìë…€ íŠ¹ë³„ê³µê¸‰ì€ ë³„ë„ì˜ ìš´ìš©ì§€ì¹¨ì— ë”°ë¼ ì‹œÂ·ë„ ë‹¨ìœ„ ìš°ì„ ê³µê¸‰ í›„ ìˆ˜ë„ê¶Œ ê±°ì£¼ìì—ê²Œ ê³µê¸‰ëœë‹¤.\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "{'answer': 'ê³µê¸‰ê·œì¹™ ì œ34ì¡°ê°€ ì ìš©ë˜ëŠ” ì§€ì—­ì— ì£¼íƒì„ ê³µê¸‰í•˜ëŠ” ê²½ìš° íŠ¹ë³„ê³µê¸‰ ë¬¼ëŸ‰ ë˜í•œ ê·¸ ê³µê¸‰ ë¹„ìœ¨ì— ë”°ë¼ ë°°ì •í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤. '\n",
      "           '(ì˜ˆ) ì„œìš¸íŠ¹ë³„ì‹œÂ·ì¸ì²œê´‘ì—­ì‹œ ëŒ€ê·œëª¨ íƒì§€ê°œë°œì§€êµ¬ì—ì„œ ê³µê¸‰ë˜ëŠ” ì£¼íƒ: íŠ¹ë³„ì‹œÂ·ê´‘ì—­ì‹œ ê±°ì£¼ì(ê±°ì£¼ê¸°ê°„ ì¶©ì¡± í•„ìš”)ì—ê²Œ '\n",
      "           'ê³µê¸‰ë¬¼ëŸ‰ì˜ 50% ìš°ì„ ê³µê¸‰ â†’ ì´í›„ ì”ì—¬ë¬¼ëŸ‰ì„ ì „ì²´ ìˆ˜ë„ê¶Œ ê±°ì£¼ìë¥¼ ëŒ€ìƒìœ¼ë¡œ ê³µê¸‰ (ì˜ˆ) ê²½ê¸°ë„ ê³¼ì²œì‹œ ëŒ€ê·œëª¨ '\n",
      "           'íƒì§€ê°œë°œì§€êµ¬ì—ì„œ ê³µê¸‰ë˜ëŠ” ì£¼íƒ: ê³¼ì²œì‹œ ê±°ì£¼ì(ê±°ì£¼ê¸°ê°„ ì¶©ì¡± í•„ìš”)ì—ê²Œ ê³µê¸‰ë¬¼ëŸ‰ì˜ 30% ìš°ì„ ê³µê¸‰ â†’ ì´í›„ ê³¼ì²œì‹œ ê³µê¸‰ '\n",
      "           'ì”ì—¬ë¬¼ëŸ‰ + 20% ê²½ê¸°ë„ ê±°ì£¼ì(ê±°ì£¼ê¸°ê°„ ì¶©ì¡± í•„ìš”)ì—ê²Œ ê³µê¸‰ â†’ ì´í›„ ì”ì—¬ë¬¼ëŸ‰ì„ ì „ì²´ ìˆ˜ë„ê¶Œ ê±°ì£¼ìë¥¼ ëŒ€ìƒìœ¼ë¡œ ê³µê¸‰ '\n",
      "           'ë‹¤ë§Œ, ë‹¤ìë…€ íŠ¹ë³„ê³µê¸‰ì˜ ê²½ìš° ã€Œë‹¤ìë…€ê°€êµ¬ ë° ë…¸ë¶€ëª¨ë¶€ì–‘ ì£¼íƒ íŠ¹ë³„ê³µê¸‰ ìš´ìš©ì§€ì¹¨ã€ ì œ5ì¡° ë‹¨ì„œì— ë”°ë¼ ìˆ˜ë„ê¶Œì—ì„œ '\n",
      "           'ì…ì£¼ìë¥¼ ëª¨ì§‘í•˜ëŠ” ë•Œì—ëŠ” í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ ì‹œÂ·êµ°Â·êµ¬ê°€ ì†í•œ ì‹œÂ·ë„ì— 50í¼ì„¼íŠ¸ë¥¼ ìš°ì„ ê³µê¸‰í•˜ê³  ë‚˜ë¨¸ì§€ ì£¼íƒ(ìš°ì„ ê³µê¸‰ì—ì„œ '\n",
      "           'ë¯¸ë¶„ì–‘ëœ ì£¼íƒì„ í¬í•¨í•œë‹¤)ì€ ìˆ˜ë„ê¶Œ ê±°ì£¼ì(ìš°ì„ ê³µê¸‰ì—ì„œ ì…ì£¼ìë¡œ ì„ ì •ë˜ì§€ ì•„ë‹ˆí•œ ìë¥¼ í¬í•¨í•œë‹¤)ì—ê²Œ ê³µê¸‰í• ìˆ˜ ìˆìŠµë‹ˆë‹¤. '\n",
      "           '(ì˜ˆ) ê²½ê¸°ë„ ê³¼ì²œì‹œ: ê²½ê¸°ë„ ê±°ì£¼ìì—ê²Œ ê³µê¸‰ë¬¼ëŸ‰ì˜ 50%ë¥¼ ìš°ì„ ê³µê¸‰í•˜ë‚˜, ê²½ê¸°ë„ ë‚´ì—ì„œ ê²½ìŸì´ ë°œìƒí•œ ê²½ìš° ê³¼ì²œì‹œ '\n",
      "           'ê±°ì£¼ì(ê±°ì£¼ê¸°ê°„ ì¶©ì¡± í•„ìš”)ì—ê²Œ ìš°ì„  ê³µê¸‰í•˜ê³  ë‚¨ì€ ë¬¼ëŸ‰ì´ ë°œìƒí•˜ëŠ” ê²½ìš° ê²½ê¸°ë„ ê±°ì£¼ìì—ê²Œ ê³µê¸‰ â†’ ì´í›„ ì”ì—¬ë¬¼ëŸ‰ì„ '\n",
      "           'ìˆ˜ë„ê¶Œ ì „ì²´ ê±°ì£¼ìë¥¼ ëŒ€ìƒìœ¼ë¡œ ê³µê¸‰',\n",
      " 'keyword': 'íŠ¹ë³„ê³µê¸‰',\n",
      " 'question': 'ã€Œì£¼íƒê³µê¸‰ì— ê´€í•œ ê·œì¹™ã€ ì œ34ì¡°ì— ë”°ë¥¸ ëŒ€ê·œëª¨íƒì§€ê°œë°œì§€êµ¬ì—ì„œ ì£¼íƒì´ ê³µê¸‰ë˜ëŠ” ê²½ìš° ì¼ë°˜ê³µê¸‰ ë¿ë§Œ ì•„ë‹ˆë¼ íŠ¹ë³„ê³µê¸‰ '\n",
      "             'ë¬¼ëŸ‰ë„ ê³µê¸‰ë¹„ìœ¨ì— ë”°ë¼ ë°°ì •ë˜ëŠ”ì§€?',\n",
      " 'question_id': 6,\n",
      " 'summary': 'ëŒ€ê·œëª¨íƒì§€ê°œë°œì§€êµ¬ì—ì„œ ì£¼íƒ ê³µê¸‰ ì‹œ íŠ¹ë³„ê³µê¸‰ ë¬¼ëŸ‰ë„ ê³µê¸‰ ë¹„ìœ¨ì— ë”°ë¼ ë°°ì •ë˜ë©°, ê±°ì£¼ì§€ë³„ ìš°ì„ ê³µê¸‰ ë¹„ìœ¨ì´ ì ìš©ëœë‹¤. '\n",
      "            'ë‹¤ìë…€ íŠ¹ë³„ê³µê¸‰ì€ ë³„ë„ì˜ ìš´ìš©ì§€ì¹¨ì— ë”°ë¼ ì‹œÂ·ë„ ë‹¨ìœ„ ìš°ì„ ê³µê¸‰ í›„ ìˆ˜ë„ê¶Œ ê±°ì£¼ìì—ê²Œ ê³µê¸‰ëœë‹¤.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def format_qa_pairs(qa_pairs):\n",
    "    \"\"\"\n",
    "    ì¶”ì¶œëœ QA ìŒì„ í¬ë§·íŒ…í•˜ì—¬ ë¬¸ì„œ ê°ì²´ë¡œ ë³€í™˜\n",
    "    \"\"\"\n",
    "    processed_docs = []\n",
    "    for pair in qa_pairs:\n",
    "        # í‚¤ì›Œë“œì™€ ìš”ì•½ ì¶”ì¶œ\n",
    "        result = keyowrd_extractor.invoke({\n",
    "            \"question\": pair['question'],\n",
    "            \"answer\": pair['answer']\n",
    "        })\n",
    "\n",
    "        # ë¬¸ì„œ ê°ì²´ ìƒì„±\n",
    "        doc = Document(\n",
    "            page_content=result.summary,\n",
    "            metadata={\n",
    "                'question_id': int(pair['number']),\n",
    "                'question': pair['question'],\n",
    "                'answer': pair['answer'],\n",
    "                'keyword': result.keyword,\n",
    "                'summary': result.summary\n",
    "            }\n",
    "        )\n",
    "        processed_docs.append(doc)\n",
    "\n",
    "    return processed_docs\n",
    "\n",
    "\n",
    "# QA ìŒ í¬ë§·íŒ…\n",
    "formatted_docs = format_qa_pairs(qa_pairs)\n",
    "print(f\"í¬ë§·íŒ…ëœ ë¬¸ì„œ ê°œìˆ˜: {len(formatted_docs)}\")\n",
    "\n",
    "# ë¬¸ì„œ í™•ì¸\n",
    "print(formatted_docs[5].page_content)\n",
    "print(\"-\" * 200)\n",
    "# ë¬¸ì„œ ë©”íƒ€ë°ì´í„° í™•ì¸\n",
    "pprint(formatted_docs[5].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í¬ë§·íŒ…ëœ ë¬¸ì„œë¥¼ data/housing_faq_formatted.jsonì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ë¬¸ì„œ ì €ì¥\n",
    "output_file = \"data/housing_faq_formatted.json\"\n",
    "with open(output_file, 'w', encoding='utf-8-sig') as f:\n",
    "    json.dump(\n",
    "        [doc.model_dump() for doc in formatted_docs], \n",
    "        f, \n",
    "        indent=2, \n",
    "        ensure_ascii=False\n",
    "    )  # í•œê¸€ì´ ìœ ë‹ˆì½”ë“œë¡œ ë³€í™˜ë˜ì§€ ì•Šë„ë¡ ì„¤ì •\n",
    "\n",
    "print(f\"í¬ë§·íŒ…ëœ ë¬¸ì„œë¥¼ {output_file}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì„œ ë¡œë“œ\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "output_file = \"data/housing_faq_formatted.json\"\n",
    "with open(output_file, 'r', encoding='utf-8-sig') as f:\n",
    "    formatted_docs = [Document(**doc) for doc in json.load(f)]\n",
    "    \n",
    "# ë¬¸ì„œ í™•ì¸\n",
    "print(formatted_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë²¡í„° ì €ì¥ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\\) Store ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_milvus import Milvus\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\"uri\": \"./milvus.db\"},\n",
    "    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë¬¸ì„œ ê²€ìƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"housing_faq_db\",\n",
    "    persist_directory=\"./chroma_db\", \n",
    "    embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\": 3, \"score_threshold\":0.05},\n",
    ")\n",
    "\n",
    "# ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ\n",
    "vector_store_with_summary = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²€ìƒ‰ê¸° ìƒì„± - ìœ ì‚¬ë„ ê¸°ë°˜ ìƒìœ„ 3ê°œ ë¬¸ì„œ ê²€ìƒ‰\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": 3},\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MMR ê²€ìƒ‰ê¸° ì •ì˜\n",
    "\n",
    "- ìš”ì•½ ë¬¸ì„œ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì‚¬ìš©\n",
    "- 10ê°œì˜ ë¬¸ì„œë¥¼ ê°€ì ¸ì™€ì„œ, ë‹¤ì–‘ì„± ê¸°ë°˜ìœ¼ë¡œ 3ê°œë¥¼ ì„ íƒ (ë‹¤ì–‘ì„±ì€ ì¤‘ê°„ ìˆ˜ì¤€ ì ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n",
    "mmr_retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 3, \"fetch_k\": 10, \"lambda_mult\": 0.5},\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "\n",
    "results = mmr_retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(result.metadata['question'])\n",
    "    print(result.metadata['answer'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **[ì‹¬í™”] ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§**\n",
    "\n",
    "- Chroma ë¬¸ì„œ: https://docs.trychroma.com/docs/querying-collections/metadata-filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¨ì¼ í•„ë“œ ì •í™•íˆ ì¼ì¹˜\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"filter\": {\"keyword\": \"ì£¼íƒê±´ì„¤ì§€ì—­\"}},\n",
    ")\n",
    "\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $eq ì—°ì‚°ì ì‚¬ìš© - ì •í™•íˆ ì¼ì¹˜\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"filter\": {\"keyword\": {\"$eq\": \"ì£¼íƒê±´ì„¤ì§€ì—­\"}}},\n",
    ")\n",
    "\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ne (Not Equal) ì—°ì‚°ì ì‚¬ìš© - ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ë¬¸ì„œ ê²€ìƒ‰\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"filter\": {\"keyword\": {\"$ne\": \"ì£¼íƒê±´ì„¤ì§€ì—­\"}}},\n",
    ")\n",
    "\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $in ì—°ì‚°ìë¡œ ì—¬ëŸ¬ ê°’ ì¤‘ ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œ ê²€ìƒ‰\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"filter\": {\"keyword\": {\"$in\": [\"ì£¼íƒê±´ì„¤ì§€ì—­\", \"ì²­ì•½ì˜ˆê¸ˆ\"]}}},\n",
    ")\n",
    "\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìˆ«ì ë²”ìœ„ ê²€ìƒ‰ ($gt, $gte, $lt, $lte) - question_idê°€ 10 ì´ìƒì¸ ë¬¸ì„œ ê²€ìƒ‰\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"filter\": {\"question_id\": {\"$gte\": 10}}},\n",
    ")\n",
    "\n",
    "query = \"ë¬´ì£¼íƒì ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $andë¡œ ì—¬ëŸ¬ ì¡°ê±´ ì¡°í•© - keywordê°€ \"ì£¼íƒê±´ì„¤ì§€ì—­\"ì´ê³  question_idê°€ 10 ë¯¸ë§Œì¸ ë¬¸ì„œ ê²€ìƒ‰\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"filter\": {\"$and\": [\n",
    "        {\"keyword\": \"ì£¼íƒê±´ì„¤ì§€ì—­\"}, \n",
    "        {\"question_id\": {\"$lt\": 10}}\n",
    "    ]}},\n",
    ")\n",
    "\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $orë¡œ ì—¬ëŸ¬ ì¡°ê±´ ì¤‘ í•˜ë‚˜ ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œ ê²€ìƒ‰ - keywordê°€ \"ì£¼íƒê±´ì„¤ì§€ì—­\"ì´ê±°ë‚˜ question_idê°€ 10 ì´ìƒì¸ ë¬¸ì„œ ê²€ìƒ‰\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"filter\": {\"$or\": [\n",
    "        {\"keyword\": \"ì£¼íƒê±´ì„¤ì§€ì—­\"}, \n",
    "        {\"question_id\": {\"$gte\": 10}}\n",
    "    ]}},\n",
    ")\n",
    "\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì •ê·œì‹ íŒ¨í„´ ë§¤ì¹­ - page_content ë³¸ë¬¸ì— \"ì£¼íƒê±´ì„¤ì§€ì—­\"ì´ í¬í•¨ëœ ë¬¸ì„œ ê²€ìƒ‰\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={'where_document': {'$contains': 'ì£¼íƒê±´ì„¤ì§€ì—­'}},\n",
    ")\n",
    "\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "class MetadataFilter(BaseModel):\n",
    "    keyword: Optional[str] = Field(description=\"ê²€ìƒ‰í•  í‚¤ì›Œë“œ\")\n",
    "    keyword_expression: Optional[str] = Field(description=\"í‚¤ì›Œë“œ ê²€ìƒ‰ í‘œí˜„ì‹\")\n",
    "    question_id: Optional[int] = Field(description=\"ì§ˆë¬¸ IDì˜ ìµœì†Œê°’\")\n",
    "    question_id_expression: Optional[str] = Field(description=\"ì§ˆë¬¸ ID ê²€ìƒ‰ í‘œí˜„ì‹\")\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"ì‚¬ìš©ì ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œì™€ ì§ˆë¬¸ ID ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ Chroma DB ê²€ìƒ‰ í•„í„°ë¥¼ ìƒì„±í•œë‹¤.\n",
    "ë‹¤ìŒ ì˜ˆì‹œë¥¼ ì°¸ì¡°í•œë‹¤:\n",
    "\n",
    "1. í‚¤ì›Œë“œ ê²€ìƒ‰ ì˜ˆì‹œ:\n",
    "- ì…ë ¥: \"ì£¼íƒê±´ì„¤ ê´€ë ¨ ë¬¸ì„œ ì°¾ì•„ì¤˜\"\n",
    "ì¶œë ¥:\n",
    "keyword: \"ì£¼íƒê±´ì„¤\"\n",
    "keyword_expression: \"$eq\"\n",
    "\n",
    "2. ì§ˆë¬¸ ID ê²€ìƒ‰ ì˜ˆì‹œ:\n",
    "- ì…ë ¥: \"ì§ˆë¬¸ ID 10ë²ˆ ì´ìƒì¸ ë¬¸ì„œ\"\n",
    "ì¶œë ¥:\n",
    "question_id: 10\n",
    "question_id_expression: \"$gte\"\n",
    "\n",
    "3. ë³µí•© ê²€ìƒ‰ ì˜ˆì‹œ:\n",
    "- ì…ë ¥: \"ì£¼íƒê±´ì„¤ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ì„œ ì§ˆë¬¸ IDê°€ 5ë²ˆì—ì„œ 15ë²ˆ ì‚¬ì´ì¸ ë¬¸ì„œ\"\n",
    "ì¶œë ¥:\n",
    "keyword: \"ì£¼íƒê±´ì„¤\"\n",
    "keyword_expression: \"$eq\"\n",
    "question_id: 5 \n",
    "question_id_expression: \"$gte\"\n",
    "\n",
    "ê²€ìƒ‰ í‘œí˜„ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•œë‹¤:\n",
    "- ë™ë“± ë¹„êµ: $eq\n",
    "- í¬ê±°ë‚˜ ê°™ìŒ: $gte\n",
    "- ì‘ê±°ë‚˜ ê°™ìŒ: $lte\n",
    "- ë²”ìœ„ ê²€ìƒ‰: $gt, $lt\n",
    "\n",
    "ìš”ì²­ì— í•´ë‹¹ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° í•´ë‹¹ í•„ë“œëŠ” nullë¡œ ë°˜í™˜í•œë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{query}\")\n",
    "])\n",
    "\n",
    "# êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ìœ„í•œ ì²´ì¸ ìƒì„±\n",
    "model_with_structure = llm.with_structured_output(MetadataFilter)\n",
    "metadata_chain = prompt | model_with_structure\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "query = \"ì£¼íƒê±´ì„¤ì§€ì—­ ê´€ë ¨ ë¬¸ì„œë¥¼ 10ë²ˆ ì´í•˜ì¸ ë¬¸ì„œì¤‘ì—ì„œ ê²€ìƒ‰í•´ì£¼ì„¸ìš”\"\n",
    "filter_params = metadata_chain.invoke({\"query\": query})\n",
    "\n",
    "print(f\"í‚¤ì›Œë“œ: {filter_params.keyword}\")\n",
    "print(f\"í‚¤ì›Œë“œ í‘œí˜„ì‹: {filter_params.keyword_expression}\")\n",
    "print(f\"ì§ˆë¬¸ ID: {filter_params.question_id}\")\n",
    "print(f\"ì§ˆë¬¸ ID í‘œí˜„ì‹: {filter_params.question_id_expression}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”íƒ€ë°ì´í„° í•„í„° ìƒì„±\n",
    "filter_dict = {}\n",
    "\n",
    "if filter_params.keyword and filter_params.question_id:\n",
    "    # ë‘ ì¡°ê±´ ëª¨ë‘ ìˆëŠ” ê²½ìš° AND ì—°ì‚°ì ì‚¬ìš©\n",
    "    filter_dict = {\n",
    "        \"$and\": [\n",
    "            {\"keyword\": {filter_params.keyword_expression: filter_params.keyword}},\n",
    "            {\"question_id\": {filter_params.question_id_expression: filter_params.question_id}}\n",
    "        ]\n",
    "    }\n",
    "elif filter_params.keyword:\n",
    "    # í‚¤ì›Œë“œ ì¡°ê±´ë§Œ ìˆëŠ” ê²½ìš°\n",
    "    filter_dict = {\"keyword\": {filter_params.keyword_expression: filter_params.keyword}}\n",
    "elif filter_params.question_id:\n",
    "    # ì§ˆë¬¸ ID ì¡°ê±´ë§Œ ìˆëŠ” ê²½ìš°\n",
    "    filter_dict = {\"question_id\": {filter_params.question_id_expression: filter_params.question_id}}\n",
    "\n",
    "# retrieverì— í•„í„° ì ìš© \n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"filter\": filter_dict} if filter_dict else {}\n",
    ")\n",
    "\n",
    "# ê²€ìƒ‰ ì‹¤í–‰\n",
    "query = \"ì£¼íƒê±´ì„¤ì§€ì—­ ê´€ë ¨ ë¬¸ì„œë¥¼ ì§ˆë¬¸ ID 10ë²ˆ ì´í•˜ì¸ ë¬¸ì„œì¤‘ì—ì„œ ê²€ìƒ‰í•´ì£¼ì„¸ìš”\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Dict\n",
    "\n",
    "\n",
    "class MetadataFilter(BaseModel):\n",
    "    keyword: Optional[str] = Field(description=\"ê²€ìƒ‰í•  í‚¤ì›Œë“œ\")\n",
    "    keyword_expression: Optional[str] = Field(description=\"í‚¤ì›Œë“œ ê²€ìƒ‰ í‘œí˜„ì‹\")\n",
    "    question_id: Optional[int] = Field(description=\"ì§ˆë¬¸ IDì˜ ìµœì†Œê°’\")\n",
    "    question_id_expression: Optional[str] = Field(description=\"ì§ˆë¬¸ ID ê²€ìƒ‰ í‘œí˜„ì‹\")\n",
    "\n",
    "\n",
    "def create_metadata_filter(query: str, llm) -> Dict:\n",
    "    \"\"\"\n",
    "    ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ Chroma DB ê²€ìƒ‰ì„ ìœ„í•œ ë©”íƒ€ë°ì´í„° í•„í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Args:\n",
    "        query: ì‚¬ìš©ì ê²€ìƒ‰ ì¿¼ë¦¬.\n",
    "        llm: ì‚¬ìš©í•  ì–¸ì–´ ëª¨ë¸.\n",
    "\n",
    "    Returns:\n",
    "        Chroma DB ê²€ìƒ‰ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í•„í„° ë”•ì…”ë„ˆë¦¬.\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"ì‚¬ìš©ì ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œì™€ ì§ˆë¬¸ ID ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ Chroma DB ê²€ìƒ‰ í•„í„°ë¥¼ ìƒì„±í•œë‹¤.\n",
    "    ë‹¤ìŒ ì˜ˆì‹œë¥¼ ì°¸ì¡°í•œë‹¤:\n",
    "\n",
    "    1. í‚¤ì›Œë“œ ê²€ìƒ‰ ì˜ˆì‹œ:\n",
    "    - ì…ë ¥: \"ì£¼íƒê±´ì„¤ ê´€ë ¨ ë¬¸ì„œ ì°¾ì•„ì¤˜\"\n",
    "    ì¶œë ¥:\n",
    "    keyword: \"ì£¼íƒê±´ì„¤\"\n",
    "    keyword_expression: \"$eq\"\n",
    "\n",
    "    2. ì§ˆë¬¸ ID ê²€ìƒ‰ ì˜ˆì‹œ:\n",
    "    - ì…ë ¥: \"ì§ˆë¬¸ ID 10ë²ˆ ì´ìƒì¸ ë¬¸ì„œ\"\n",
    "    ì¶œë ¥:\n",
    "    question_id: 10\n",
    "    question_id_expression: \"$lte\"\n",
    "\n",
    "    3. ë³µí•© ê²€ìƒ‰ ì˜ˆì‹œ:\n",
    "    - ì…ë ¥: \"ì£¼íƒê±´ì„¤ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ì„œ ì§ˆë¬¸ IDê°€ 5ë²ˆì—ì„œ 15ë²ˆ ì‚¬ì´ì¸ ë¬¸ì„œ\"\n",
    "    ì¶œë ¥:\n",
    "    keyword: \"ì£¼íƒê±´ì„¤\"\n",
    "    keyword_expression: \"$eq\"\n",
    "    question_id: 5\n",
    "    question_id_expression: \"$gte\"\n",
    "\n",
    "    ê²€ìƒ‰ í‘œí˜„ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•œë‹¤:\n",
    "    - ë™ë“± ë¹„êµ: $eq\n",
    "    - í¬ê±°ë‚˜ ê°™ìŒ: $gte\n",
    "    - ì‘ê±°ë‚˜ ê°™ìŒ: $lte\n",
    "    - ë²”ìœ„ ê²€ìƒ‰: $gt, $lt\n",
    "\n",
    "    ìš”ì²­ì— í•´ë‹¹ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° í•´ë‹¹ í•„ë“œëŠ” nullë¡œ ë°˜í™˜í•œë‹¤.\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{query}\")\n",
    "    ])\n",
    "\n",
    "    model_with_structure = llm.with_structured_output(MetadataFilter)\n",
    "    metadata_chain = prompt | model_with_structure\n",
    "\n",
    "    filter_params = metadata_chain.invoke({\"query\": query})\n",
    "\n",
    "    filter_dict = {}\n",
    "\n",
    "    if filter_params.keyword and filter_params.question_id:\n",
    "        filter_dict = {\n",
    "            \"$and\": [\n",
    "                {\"keyword\": {filter_params.keyword_expression: filter_params.keyword}},\n",
    "                {\"question_id\": {filter_params.question_id_expression: filter_params.question_id}}\n",
    "            ]\n",
    "        }\n",
    "    elif filter_params.keyword:\n",
    "        filter_dict = {\"keyword\": {filter_params.keyword_expression: filter_params.keyword}}\n",
    "    elif filter_params.question_id:\n",
    "        filter_dict = {\"question_id\": {filter_params.question_id_expression: filter_params.question_id}}\n",
    "\n",
    "    return filter_dict\n",
    "\n",
    "\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ \n",
    "query = \"ì£¼íƒê±´ì„¤ì§€ì—­ ê´€ë ¨ ë¬¸ì„œë¥¼ 10ë²ˆ ì´í•˜ì¸ ë¬¸ì„œì¤‘ì—ì„œ ê²€ìƒ‰í•´ì£¼ì„¸ìš”\"\n",
    "\n",
    "@chain\n",
    "def metadata_filter_query(query: str):\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        model='gpt-4.1-mini',\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    filter_dict = create_metadata_filter(query, llm)\n",
    "    print(filter_dict)\n",
    "\n",
    "    retriever = vector_store.as_retriever(\n",
    "            search_kwargs={\"filter\": filter_dict} if filter_dict else {}\n",
    "    )\n",
    "    results = retriever.invoke(query)\n",
    "\n",
    "    return results\n",
    "\n",
    "results = metadata_filter_query.invoke(query)\n",
    "\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ë¥¸ ì¿¼ë¦¬\n",
    "query = \"ì²­ì•½í†µì¥ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”\"\n",
    "results = metadata_filter_query.invoke(query)\n",
    "\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) ì°¸ì¡° ë¬¸ì„œ ì—†ì´ ì§ì ‘ ë‹µë³€ì„ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Prompt\n",
    "template = '''Answer the question based only on the following context.\n",
    "\n",
    "[Context]\n",
    "{context}\n",
    "\n",
    "[Question]\n",
    "{question}\n",
    "\n",
    "[Answer (in í•œêµ­ì–´)]\n",
    "'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "# ë¬¸ì„œ í¬ë§·íŒ…\n",
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "# ê²€ìƒ‰ê¸° ìƒì„± - ìœ ì‚¬ë„ ê¸°ë°˜ ìƒìœ„ 3ê°œ ë¬¸ì„œ ê²€ìƒ‰\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": 3},\n",
    ")\n",
    "\n",
    "\n",
    "# Chain êµ¬ì„±\n",
    "rag_chain = (\n",
    "    {'context': retriever | format_docs, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Chain ì‹¤í–‰\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) ì°¸ì¡° ë¬¸ì„œë¥¼ ë‹µë³€ê³¼ í•¨ê»˜ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) ë¬¸ì„œì™€ í¬ë§·íŒ…ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ìŒ ì½”ë“œë¥¼ ì™„ì„±í•˜ì„¸ìš”.\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "def get_context_and_docs(question: str) -> Dict:\n",
    "    \"\"\"ë¬¸ì„œì™€ í¬ë§·íŒ…ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ë°˜í™˜\n",
    "    \n",
    "    Args:\n",
    "        question: ê²€ìƒ‰í•  ì§ˆë¬¸\n",
    "\n",
    "    Returns:\n",
    "        Dict: ë¬¸ì„œì™€ í¬ë§·íŒ…ëœ ì»¨í…ìŠ¤íŠ¸, ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "\n",
    "    # ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°\n",
    "    docs = retriever.invoke(question)\n",
    "    return {\n",
    "        \"question\": None,  # ì§ˆë¬¸\n",
    "        \"context\": None,   # ë¬¸ì„œ í¬ë§·íŒ…ëœ ì»¨í…ìŠ¤íŠ¸\n",
    "        \"source_documents\": None   # ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ìŒ ì½”ë“œë¥¼ ì™„ì„±í•˜ì„¸ìš”.\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def prompt_and_generate_answer(input_data: Dict) -> Dict:\n",
    "    \"\"\"ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ë‹µë³€ì„ ìƒì„±\n",
    "\n",
    "    Args:\n",
    "        input_data (Dict): ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì´ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬\n",
    "\n",
    "    Returns:\n",
    "        Dict: ìƒì„±ëœ ë‹µë³€ê³¼ ì†ŒìŠ¤ ë¬¸ì„œ ì •ë³´ê°€ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬\n",
    "    \"\"\"\n",
    "\n",
    "    # LCEL ì²´ì¸ êµ¬ì„± (StrOutputParser ì‚¬ìš©)\n",
    "    answer_chain = None\n",
    "\n",
    "    return {\n",
    "        \"answer\": None,  # ìƒì„±ëœ ë‹µë³€ (answer_chain ê²°ê³¼)\n",
    "        \"source_documents\": None  # ì†ŒìŠ¤ ë¬¸ì„œ ì •ë³´ (input_dataì—ì„œ ê°€ì ¸ì˜´)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) RAG ì²´ì¸ êµ¬ì„±`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ìŒ ì½”ë“œë¥¼ ì™„ì„±í•˜ì„¸ìš”.\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "# Chain êµ¬ì„±\n",
    "rag_chain = (\n",
    "    None(get_context_and_docs) |  # ë¬¸ì„œì™€ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° \n",
    "    {\n",
    "        'response': None(prompt_and_generate_answer), # ë‹µë³€ ìƒì„±\n",
    "        'question': None(),\n",
    "        \"source_documents\": None(\"source_documents\")   # ì†ŒìŠ¤ ë°˜í™˜\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain ì‹¤í–‰\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "result = rag_chain.invoke(query)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"ë‹µë³€:\", result[\"response\"][\"answer\"])\n",
    "print(\"\\nì°¸ì¡° ë¬¸ì„œ:\")\n",
    "for i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "    print(f\"\\në¬¸ì„œ {i}:\")\n",
    "    print(f\"ë‚´ìš©: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) ê²€ìƒ‰ ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) ê²€ìƒ‰ ë¬¸ì„œì™€ ì§ˆë¬¸ ê°„ì˜ ê´€ë ¨ì„±ì„ í‰ê°€`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²€ìƒ‰ ë¬¸ì„œì˜ ì§ˆë¬¸ ê´€ë ¨ì„± í‰ê°€\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° í•„ìš”í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ ë…¼ë¦¬ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.\n",
    "ë‹¨ê³„ì ìœ¼ë¡œ ì§„í–‰í•˜ë©°, í‰ê°€ê²°ê³¼ì— ëŒ€í•œ ê²€ì¦ì„ ìˆ˜í–‰í•˜ì„¸ìš”.\n",
    "\n",
    "ë‹¤ìŒ ê¸°ì¤€ ì¤‘ í•˜ë‚˜ ì´ìƒì„ ì¶©ì¡±í•  ê²½ìš° 'Yes'ë¡œ ë‹µë³€í•˜ê³ , ëª¨ë‘ ì¶©ì¡±í•˜ì§€ ëª»í•˜ë©´ 'No'ë¡œ ë‹µë³€í•˜ì„¸ìš”:\n",
    "\n",
    "1. ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° í•„ìš”í•œ ì •ë³´ë¥¼ ì§ì ‘ì ìœ¼ë¡œ í¬í•¨í•˜ê³  ìˆëŠ”ê°€?\n",
    "2. ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ë¡œë¶€í„° ë‹µë³€ì— í•„ìš”í•œ ë‚´ìš©ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ”ê°€?\n",
    "3. ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ê°€ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆëŠ”ê°€?\n",
    "\n",
    "'Yes' ë˜ëŠ” 'No'ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\"\"\"),\n",
    "    (\"human\", \"\"\"[ì»¨í…ìŠ¤íŠ¸]\n",
    "{context}\n",
    "\n",
    "[ì§ˆë¬¸]\n",
    "{question}\"\"\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()    # gpt-4.1-mini ëª¨ë¸ ì‚¬ìš©\n",
    "\n",
    "for i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "    print(f\"\\në¬¸ì„œ {i}:\")\n",
    "    print(f\"ë‚´ìš©: {doc.page_content}\")\n",
    "    relevance = chain.invoke({\n",
    "        \"context\": doc.page_content,\n",
    "        \"question\": query\n",
    "    }).lower()\n",
    "\n",
    "    print(f\"í‰ê°€ ê²°ê³¼: {relevance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-4.1 ëª¨ë¸ ì‚¬ìš©\n",
    "\n",
    "llm_gpt4o = ChatOpenAI(\n",
    "    model='gpt-4.1',\n",
    "    temperature=0.1,\n",
    "    top_p=0.9, \n",
    ")\n",
    "\n",
    "chain = prompt | llm_gpt4o | StrOutputParser()    # gpt-4.1 ëª¨ë¸ ì‚¬ìš©\n",
    "\n",
    "for i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "    print(f\"\\në¬¸ì„œ {i}:\")\n",
    "    print(f\"ë‚´ìš©: {doc.page_content}\")\n",
    "    relevance = chain.invoke({\n",
    "        \"context\": doc.page_content,\n",
    "        \"question\": query\n",
    "    }).lower()\n",
    "\n",
    "    print(f\"í‰ê°€ ê²°ê³¼: {relevance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradio ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ\n",
    "vector_sotre = Chroma(\n",
    "    collection_name=\"housing_faq_db\",\n",
    "    persist_directory=\"./chroma_db\", \n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "\n",
    "# ê²€ìƒ‰ê¸° ìƒì„± - ìœ ì‚¬ë„ ê¸°ë°˜ ìƒìœ„ 3ê°œ ë¬¸ì„œ ê²€ìƒ‰\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": 3},\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class SearchResult:\n",
    "    context: str\n",
    "    source_documents: Optional[List]\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(\n",
    "            self, \n",
    "            llm: BaseChatModel, \n",
    "            eval_llm: BaseChatModel,\n",
    "            retriever: VectorStoreRetriever\n",
    "        ):\n",
    "        if not llm:\n",
    "            self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "        else:\n",
    "            self.llm = llm\n",
    "\n",
    "        if not eval_llm:\n",
    "            self.eval_llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "        else:\n",
    "            self.eval_llm = eval_llm\n",
    "\n",
    "        if not retriever:\n",
    "            raise ValueError(\"ê²€ìƒ‰ê¸°(retriever)ê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "        else:\n",
    "            self.retriever = retriever\n",
    "        \n",
    "    def _format_docs(self, docs: List) -> str:\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    def _format_source_documents(self, docs: Optional[List]) -> str:\n",
    "        if not docs:\n",
    "            return \"\\n\\nâ„¹ï¸ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "        \n",
    "        formatted_docs = []\n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            metadata = doc.metadata if hasattr(doc, 'metadata') else {}\n",
    "            source_info = []\n",
    "            \n",
    "            if 'question_id' in metadata:\n",
    "                source_info.append(f\"ID: {metadata['question_id']}\")\n",
    "            if 'keyword' in metadata:\n",
    "                source_info.append(f\"í‚¤ì›Œë“œ: {metadata['keyword']}\")\n",
    "            if 'summary' in metadata:\n",
    "                source_info.append(f\"ìš”ì•½: {metadata['summary']}\")\n",
    "                \n",
    "            formatted_docs.append(\n",
    "                f\"ğŸ“š ì°¸ì¡° ë¬¸ì„œ {i}\\n\"\n",
    "                f\"â€¢ {' | '.join(source_info) if source_info else 'ì¶œì²˜ ì •ë³´ ì—†ìŒ'}\\n\"\n",
    "                f\"â€¢ ë‚´ìš©: {doc.page_content}\"\n",
    "            )\n",
    "        \n",
    "        return \"\\n\\n\" + \"\\n\\n\".join(formatted_docs)\n",
    "    \n",
    "    def _check_relevance(self, docs: List, question: str) -> List:\n",
    "        \"\"\"ë¬¸ì„œì˜ ê´€ë ¨ì„± í™•ì¸\"\"\"\n",
    "\n",
    "        relevant_docs = []\n",
    "\n",
    "        if not docs:\n",
    "            return relevant_docs\n",
    "            \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° í•„ìš”í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.\n",
    "\n",
    "        ë‹¤ìŒ ê¸°ì¤€ ì¤‘ í•˜ë‚˜ ì´ìƒì„ ì¶©ì¡±í•  ê²½ìš° 'Yes'ë¡œ ë‹µë³€í•˜ê³ , ëª¨ë‘ ì¶©ì¡±í•˜ì§€ ëª»í•˜ë©´ 'No'ë¡œ ë‹µë³€í•˜ì„¸ìš”:\n",
    "\n",
    "        1. ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° í•„ìš”í•œ ì •ë³´ë¥¼ ì§ì ‘ì ìœ¼ë¡œ í¬í•¨í•˜ê³  ìˆëŠ”ê°€?\n",
    "        2. ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ë¡œë¶€í„° ë‹µë³€ì— í•„ìš”í•œ ë‚´ìš©ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ”ê°€?\n",
    "\n",
    "        'Yes' ë˜ëŠ” 'No'ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\"\"\"),\n",
    "            (\"human\", \"\"\"[ì»¨í…ìŠ¤íŠ¸]\n",
    "        {context}\n",
    "\n",
    "        [ì§ˆë¬¸]\n",
    "        {question}\"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | self.eval_llm | StrOutputParser()\n",
    "\n",
    "        for doc in docs:\n",
    "            result = chain.invoke({\n",
    "                \"context\": doc.page_content,\n",
    "                \"question\": question\n",
    "            }).lower()\n",
    "\n",
    "            print(f\"ë¬¸ì„œ {doc.metadata['question_id']} ê´€ë ¨ì„± í™•ì¸ ê²°ê³¼: {result}\")\n",
    "            print(f\"ë¬¸ì„œ {doc.metadata['question_id']} ë‚´ìš©:\")\n",
    "            print(doc.page_content)\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            if \"yes\" in result:\n",
    "                relevant_docs.append(doc)\n",
    "            \n",
    "        return relevant_docs\n",
    "    \n",
    "    def search_documents(self, question: str) -> SearchResult:\n",
    "        try:\n",
    "            docs = retriever.invoke(question)\n",
    "            print(f\"ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜: {len(docs)}\")\n",
    "            relevant_docs = self._check_relevance(docs, question) \n",
    "            print(f\"ê´€ë ¨ ë¬¸ì„œ ê°œìˆ˜: {len(relevant_docs)}\")\n",
    "            \n",
    "            return SearchResult(\n",
    "                context=self._format_docs(relevant_docs) if relevant_docs else \"ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\",\n",
    "                source_documents=relevant_docs,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            return SearchResult(\n",
    "                context=\"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\",\n",
    "                source_documents=None,\n",
    "            )\n",
    "    \n",
    "    def generate_answer(self, message: str, history: List) -> str:\n",
    "        # ë¬¸ì„œ ê²€ìƒ‰\n",
    "        search_result = self.search_documents(message)\n",
    "        \n",
    "        if not search_result.source_documents:\n",
    "            return \"ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?\"\n",
    "                    \n",
    "        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:\n",
    "            1. ì£¼ì–´ì§„ ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "            2. ë¬¸ì„œì— ëª…í™•í•œ ê·¼ê±°ê°€ ì—†ëŠ” ë‚´ìš©ì€ \"ê·¼ê±° ì—†ìŒ\"ì´ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.\n",
    "            3. ë‹µë³€í•˜ê¸° ì–´ë ¤ìš´ ì§ˆë¬¸ì€ \"ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤\"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.\n",
    "            4. ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ì ì¸ ì§€ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\"\"\"),\n",
    "            (\"human\", \"ë¬¸ì„œë“¤:\\n{context}\\n\\nì§ˆë¬¸: {question}\")\n",
    "        ])\n",
    "        \n",
    "        # RAG Chain êµ¬ì„±\n",
    "        chain = prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        try:\n",
    "            # ë‹µë³€ ìƒì„±\n",
    "            answer = chain.invoke({\n",
    "                \"context\": search_result.context,\n",
    "                \"question\": message\n",
    "            })\n",
    "            \n",
    "            # ì°¸ì¡° ë¬¸ì„œ í¬ë§·íŒ… ì¶”ê°€\n",
    "            sources = self._format_source_documents(search_result.source_documents)\n",
    "            return f\"{answer}\\n{sources}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\"\n",
    "\n",
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •\n",
    "\n",
    "rag_system = RAGSystem(\n",
    "    llm=ChatOpenAI(model=\"gpt-4.1\", temperature=0),   \n",
    "    eval_llm=ChatOpenAI(model=\"gpt-4.1\", temperature=0),\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
    ")\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=rag_system.generate_answer,\n",
    "    title=\"RAG QA ì‹œìŠ¤í…œ\",\n",
    "    description=\"\"\"\n",
    "    ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    ëª¨ë“  ë‹µë³€ì—ëŠ” ì°¸ì¡°í•œ ë¬¸ì„œì˜ ì¶œì²˜ê°€ í‘œì‹œë©ë‹ˆë‹¤.\n",
    "    \"\"\",\n",
    "    theme=gr.themes.Soft(\n",
    "        primary_hue=\"blue\",\n",
    "        secondary_hue=\"gray\",\n",
    "    ),\n",
    "examples=[\n",
    "    [\"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"],\n",
    "    [\"ë¬´ì£¼íƒ ì„¸ëŒ€ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"],\n",
    "    [\"2ìˆœìœ„ë¡œ ë‹¹ì²¨ëœ ì‚¬ëŒì´ ì²­ì•½í†µì¥ì„ ë‹¤ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆë‚˜ìš”?\"],\n",
    "],\n",
    ")\n",
    "\n",
    "# ë°ëª¨ ì‹¤í–‰\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ì¢…ë£Œ\n",
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
