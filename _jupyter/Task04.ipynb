{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2eb7b5f",
   "metadata": {},
   "source": [
    "# LLM 지식 탐색왕 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed805db8",
   "metadata": {},
   "source": [
    "## 1. Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56875714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() # 환경 변수 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ba96d7",
   "metadata": {},
   "source": [
    "## 2. Document Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a864d09",
   "metadata": {},
   "source": [
    "### 1\\) Load Document\n",
    "- **BART** 논문을 로드함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a0a6f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 논문 URL 리스트\n",
    "urls = {\n",
    "    \"Transformer\": \"https://arxiv.org/pdf/1706.03762\", \n",
    "    \"BERT\": \"https://arxiv.org/pdf/1810.04805\", \n",
    "    \"XLNet\": \"https://arxiv.org/pdf/1906.08237\", \n",
    "    \"BART\": \"https://arxiv.org/pdf/1910.13461.pdf\", \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa61abba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF 문서 개수: 15\n",
      "PDF 문서 개수: 16\n",
      "PDF 문서 개수: 18\n",
      "PDF 문서 개수: 10\n",
      "총 문서 개수: 59\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "documents = []\n",
    "for source, url in urls.items():\n",
    "    file_name = f\"data/{source}.pdf\"\n",
    "    # PDF 다운로드\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        f.write(requests.get(url).content)\n",
    "\n",
    "    # 로컬에서 PDF 로드\n",
    "    loader = PyPDFLoader(file_name)\n",
    "    docs = loader.load()\n",
    "    print(f'PDF 문서 개수: {len(docs)}')\n",
    "\n",
    "    documents += docs\n",
    "\n",
    "print(f'총 문서 개수: {len(documents)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acbc0c0",
   "metadata": {},
   "source": [
    "### 2\\) Split Texts\n",
    "\n",
    "- **Semantic Chunking** 방식으로 텍스트를 분할함<br>\n",
    "    → 임베딩 벡터 간의 **기울기(gradient)** 변화를 기준으로 의미 단위(semantic unit)를 구분함<br>\n",
    "    → 청크 길이에 일관성이 없으며, 문맥에 따라 길이가 유동적으로 결정됨\n",
    "\n",
    "- 길이가 100자 미만인 청크는 이미지 기반 텍스트(OCR 등)로 간주하여 제거함<br>\n",
    "    → 주요 텍스트가 아닌 부가 정보일 가능성이 높기 때문임\n",
    "\n",
    "- 1차 분할된 청크는 길이 편차가 크므로, 문자열 길이 기준으로 재귀적으로 분할하여 최종적으로는 일관된 길이의 청크를 구성함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47593268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 청크 수: 178\n",
      "각 청크의 길이: [1741, 1117, 294, 137, 3824, 1783, 42, 184, 2320, 531, 528, 2127, 772, 853, 1852, 237, 227, 2856, 820, 2372, 52, 2920, 158, 2950, 2, 108, 94, 2969, 38, 2, 71, 64, 2813, 262, 693, 118, 812, 2, 133, 684, 359, 3594, 233, 3729, 562, 30, 194, 3473, 238, 3405, 1204, 275, 2495, 1070, 517, 3860, 68, 2127, 1575, 10, 288, 4172, 17, 127, 2502, 1479, 390, 180, 2076, 504, 132, 184, 1076, 732, 791, 140, 25, 1941, 113, 1027, 160, 147, 1371, 2217, 39, 125, 3535, 3344, 525, 199, 25, 1724, 627, 13, 2588, 517, 166, 2999, 256, 952, 171, 3052, 140, 1214, 2956, 3693, 1, 567, 871, 2728, 194, 2681, 1437, 2324, 1181, 1, 448, 2609, 330, 3321, 1, 108, 2343, 176, 1314, 2, 72, 611, 2429, 2, 298, 2497, 197, 2500, 344, 2751, 2608, 56, 2, 1632, 46, 1270, 1243, 380, 1973, 2279, 3429, 108, 511, 7, 8, 34, 4303, 3184, 1086, 133, 770, 108, 3245, 2897, 568, 481, 257, 1287, 723, 2615, 1152, 1516, 1298, 489, 750, 1123, 771, 1099, 662, 127, 156, 1796]\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker \n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "text_splitter = SemanticChunker(\n",
    "    embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    "    breakpoint_threshold_type=\"gradient\",  # 임계값 타입 설정 (gradient, percentile, standard_deviation, interquartile)\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"생성된 청크 수: {len(chunks)}\")\n",
    "print(f\"각 청크의 길이: {list(len(chunk.page_content) for chunk in chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d4a91da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6: The output is computed as a weighted sum\n",
      "3\n",
      "20: Table 3: Variations on the Transformer architecture.\n",
      "24: 10\n",
      "26: Learning phrase representations using rnn encoder-decoder for statistical\n",
      "machine translation.\n",
      "28: arXiv preprint arXiv:1508.04025, 2015.\n",
      "29: 11\n",
      "30: [25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini.\n",
      "31: Building a large annotated\n",
      "corpus of english: The penn treebank.\n",
      "37: 14\n",
      "45: BERT BERT\n",
      "E[CLS] E1  E[SEP]...\n",
      "56: (2018), but the system\n",
      "has improved substantially after publication.\n",
      "59: Additional\n",
      "62: (2018b) presented\n",
      "76: Curran As-\n",
      "sociates, Inc.\n",
      "84: BERT (Ours)\n",
      "Trm Trm Trm\n",
      "Trm Trm Trm\n",
      "...\n",
      "90: BERT\n",
      "E[CLS] E1  E[SEP]...\n",
      "93: jority class.\n",
      "106: 4\n",
      "115: 7\n",
      "120: 9\n",
      "125: 10\n",
      "126: [26] Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.\n",
      "129: 11\n",
      "137: Rows and columns represent query and key respec-\n",
      "tively.\n",
      "138: 15\n",
      "140: 5https://openreview.net/forum?id=HJePno0cYm\n",
      "16\n",
      "149: A B C .\n",
      "150: D E .A .\n",
      "151: C . E . A _ . D _ E . A _C . _ E .\n",
      "생성된 청크 수: 149\n",
      "각 청크의 길이: [1741, 1117, 294, 137, 3824, 1783, 184, 2320, 531, 528, 2127, 772, 853, 1852, 237, 227, 2856, 820, 2372, 2920, 158, 2950, 108, 2969, 2813, 262, 693, 118, 812, 133, 684, 359, 3594, 233, 3729, 562, 194, 3473, 238, 3405, 1204, 275, 2495, 1070, 517, 3860, 2127, 1575, 288, 4172, 127, 2502, 1479, 390, 180, 2076, 504, 132, 184, 1076, 732, 791, 140, 1941, 113, 1027, 160, 147, 1371, 2217, 125, 3535, 3344, 525, 199, 1724, 627, 2588, 517, 166, 2999, 256, 952, 171, 3052, 140, 1214, 2956, 3693, 567, 871, 2728, 194, 2681, 1437, 2324, 1181, 448, 2609, 330, 3321, 108, 2343, 176, 1314, 611, 2429, 298, 2497, 197, 2500, 344, 2751, 2608, 1632, 1270, 1243, 380, 1973, 2279, 3429, 108, 511, 4303, 3184, 1086, 133, 770, 108, 3245, 2897, 568, 481, 257, 1287, 723, 2615, 1152, 1516, 1298, 489, 750, 1123, 771, 1099, 662, 127, 156, 1796]\n"
     ]
    }
   ],
   "source": [
    "# 불필요한 청크 제거\n",
    "selected_chunks = []\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    content = chunk.page_content\n",
    "    if len(chunk.page_content) < 100:\n",
    "        print(f'{idx}: {content}')\n",
    "    else:\n",
    "        selected_chunks.append(chunk)\n",
    "\n",
    "print(f\"생성된 청크 수: {len(selected_chunks)}\")\n",
    "print(f\"각 청크의 길이: {list(len(chunk.page_content) for chunk in selected_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5f0fc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 텍스트 청크 수: 285\n",
      "각 청크의 길이: [985, 756, 1117, 294, 137, 1500, 1411, 913, 1223, 560, 184, 1242, 1078, 531, 528, 1181, 946, 772, 853, 1330, 522, 237, 227, 1297, 1032, 527, 604, 216, 1113, 1166, 93, 614, 937, 1275, 94, 158, 823, 1388, 739, 108, 830, 891, 926, 322, 937, 828, 879, 169, 262, 693, 118, 476, 336, 133, 509, 175, 359, 1052, 1062, 1248, 232, 233, 1203, 1030, 1085, 411, 562, 194, 159, 1063, 1153, 985, 111, 238, 1124, 1187, 1003, 91, 1043, 160, 275, 1234, 1122, 139, 1006, 64, 414, 103, 1040, 1014, 1011, 794, 637, 786, 704, 1007, 568, 288, 1068, 1060, 1142, 902, 127, 1204, 718, 580, 1180, 299, 390, 180, 1027, 1049, 504, 132, 184, 1008, 68, 732, 791, 140, 983, 958, 113, 928, 99, 160, 147, 973, 398, 1178, 1039, 125, 1148, 1040, 1058, 289, 1024, 1021, 1131, 168, 525, 199, 465, 1112, 145, 627, 1058, 945, 585, 517, 166, 1159, 1131, 709, 256, 952, 171, 1437, 1224, 390, 140, 772, 442, 1228, 1042, 686, 694, 1084, 1085, 829, 567, 871, 1128, 1216, 384, 194, 1170, 1111, 400, 855, 582, 995, 794, 535, 1181, 448, 599, 602, 935, 473, 330, 588, 949, 1246, 538, 108, 943, 1000, 400, 176, 844, 470, 611, 891, 771, 767, 298, 755, 641, 1100, 197, 1109, 1052, 339, 344, 925, 970, 856, 1282, 1227, 99, 766, 866, 523, 747, 504, 739, 380, 1132, 841, 1158, 1121, 580, 1239, 1149, 458, 108, 511, 1095, 1130, 1200, 878, 354, 1226, 1020, 581, 1086, 133, 494, 276, 108, 1280, 1172, 793, 366, 583, 889, 977, 82, 568, 481, 257, 1058, 229, 723, 1191, 1119, 305, 1152, 1120, 396, 1278, 20, 489, 750, 982, 141, 771, 884, 215, 662, 127, 156, 950, 846]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 청크 일정한 사이즈로 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300,                      \n",
    "    chunk_overlap=0,\n",
    "    encoding_name=\"cl100k_base\",  # TikToken 인코더 이름\n",
    "    separators=[\" \\n\", \".\\n\", r\"(?<=[.!?])\\s+\"], # 구분자\n",
    "    is_separator_regex=True,      # 구분자가 정규식인지 여부\n",
    "    keep_separator=True,          # 구분자 유지 여부\n",
    ")\n",
    "final_chunks = text_splitter.split_documents(selected_chunks)\n",
    "print(f\"생성된 텍스트 청크 수: {len(final_chunks)}\")\n",
    "print(f\"각 청크의 길이: {list(len(chunk.page_content) for chunk in final_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3178dd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author': '',\n",
      " 'creationdate': '2024-04-10T21:11:43+00:00',\n",
      " 'creator': 'LaTeX with hyperref',\n",
      " 'keywords': '',\n",
      " 'moddate': '2024-04-10T21:11:43+00:00',\n",
      " 'page': 0,\n",
      " 'page_label': '1',\n",
      " 'producer': 'pdfTeX-1.40.25',\n",
      " 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live '\n",
      "                    '2023) kpathsea version 6.3.5',\n",
      " 'source': 'data/Transformer.pdf',\n",
      " 'subject': '',\n",
      " 'title': '',\n",
      " 'total_pages': 15,\n",
      " 'trapped': '/False'}\n",
      "('∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs '\n",
      " 'with self-attention and started\\n'\n",
      " 'the effort to evaluate this idea. Ashish, with Illia, designed and '\n",
      " 'implemented the first Transformer models and\\n'\n",
      " 'has been crucially involved in every aspect of this work. Noam proposed '\n",
      " 'scaled dot-product attention, multi-head\\n'\n",
      " 'attention and the parameter-free position representation and became the '\n",
      " 'other person involved in nearly every\\n'\n",
      " 'detail. Niki designed, implemented, tuned and evaluated countless model '\n",
      " 'variants in our original codebase and\\n'\n",
      " 'tensor2tensor. Llion also experimented with novel model variants, was '\n",
      " 'responsible for our initial codebase, and\\n'\n",
      " 'efficient inference and visualizations. Lukasz and Aidan spent countless '\n",
      " 'long days designing various parts of and\\n'\n",
      " 'implementing tensor2tensor, replacing our earlier codebase, greatly '\n",
      " 'improving results and massively accelerating\\n'\n",
      " 'our research. †Work performed while at Google Brain. ‡Work performed while '\n",
      " 'at Google Research. 31st Conference on Neural Information Processing Systems '\n",
      " '(NIPS 2017), Long Beach, CA, USA. arXiv:1706.03762v7  [cs.CL]  2 Aug 2023')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(final_chunks[2].metadata)\n",
    "pprint(final_chunks[2].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51f5012",
   "metadata": {},
   "source": [
    "### 3\\) Embedding\n",
    "- 문서 임베딩은 `OpenAI`의 **text-embedding-3-small** 모델을 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e461789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 벡터의 개수: 285\n",
      "임베딩 벡터의 차원: 1024\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    dimensions=1024\n",
    ")\n",
    "document_embeddings = embeddings_model.embed_documents(\n",
    "    [chunk.page_content for chunk in final_chunks]\n",
    ")\n",
    "print(f\"임베딩 벡터의 개수: {len(document_embeddings)}\")\n",
    "print(f\"임베딩 벡터의 차원: {len(document_embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ebf505",
   "metadata": {},
   "source": [
    "### 4\\) Save Vectors\n",
    "\n",
    "- 임베딩된 벡터는 벡터스토어로 `ChromaDB` 사용하여 저장함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "227cc9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285개의 문서가 성공적으로 벡터 저장소에 추가되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "chroma_db = Chroma(\n",
    "    collection_name=\"papers\",\n",
    "    embedding_function=embeddings_model,\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    collection_metadata = {'hnsw:space': 'cosine'},\n",
    ")\n",
    "\n",
    "# 문서를 벡터 저장소에 저장\n",
    "doc_ids = [f\"DOC_{i}\" for i in range(len(final_chunks))]\n",
    "chroma_db.delete(ids=doc_ids)\n",
    "added_doc_ids = chroma_db.add_documents(documents=final_chunks, ids=doc_ids)\n",
    "print(f\"{len(added_doc_ids)}개의 문서가 성공적으로 벡터 저장소에 추가되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7281d0",
   "metadata": {},
   "source": [
    "## 3. Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694df099",
   "metadata": {},
   "source": [
    "### 1\\) Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66875faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunhwaryu/Documents/llm-study/prj02/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset.persona import Persona\n",
    "\n",
    "# 페르소나 정의 (다양한 관점에서 질문 생성)\n",
    "personas = [\n",
    "    Persona(\n",
    "        name=\"graduate_researcher\",  # 박사과정 연구원: 심도 있는 분석적 질문\n",
    "        role_description=\"As a PhD researcher studying LLM, I am delving deep into the LLM structure, pre-training and fine-tuning objectives.\",\n",
    "    ),\n",
    "    Persona(\n",
    "        name=\"junior_developer\",   # 주니어 개발자: 실무 중심적 질문\n",
    "        role_description=\"As a junior developer developing LLM services at a platform company, I am very interested in practical technical information and values.\",\n",
    "    ),\n",
    "    Persona(\n",
    "        name=\"undergraduate_student\",  # 학부생: 기초적인 학습 질문\n",
    "        role_description=\"As an undergraduate student majoring in AI, I would like to acquire basic knowledge about LLM technologies and trends.\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a5873c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# LLM과 임베딩 모델 초기화\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365c5f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying CustomNodeFilter:   0%|          | 0/285 [00:00<?, ?it/s]          Node 37881f86-c201-487c-b464-379584623a32 does not have a summary. Skipping filtering.\n",
      "Node ec12dfdc-8e3f-450b-be8a-078ebb145721 does not have a summary. Skipping filtering.\n",
      "Node f173bba4-b7f4-4e20-9bc9-975173558d6b does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:   1%|▏         | 4/285 [00:00<00:50,  5.60it/s]Node 3b33bd44-f3a8-4121-8204-e6c50e2c0997 does not have a summary. Skipping filtering.\n",
      "Node 2a5acefd-14e6-4508-9e8f-a9dec394ecc7 does not have a summary. Skipping filtering.\n",
      "Node 0b951e51-7eb4-4101-bda1-c05e6c9f768f does not have a summary. Skipping filtering.\n",
      "Node 22be5b0c-3716-4f0e-b3e3-59a473f4d928 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:   6%|▌         | 17/285 [00:00<00:10, 25.97it/s]Node 146c8e0a-4077-416b-8a10-36a4de365443 does not have a summary. Skipping filtering.\n",
      "Node 8bf0096c-ab34-4683-b894-2b34763fe979 does not have a summary. Skipping filtering.\n",
      "Node a3ebe786-f13d-42f7-847d-4aa44f7c7ccc does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:   9%|▉         | 26/285 [00:00<00:07, 35.79it/s]Node f692d7aa-3ad8-4fd3-990d-c834807adfef does not have a summary. Skipping filtering.\n",
      "Node 906365fc-c6de-40a8-b3ff-6b75025f63c9 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  12%|█▏        | 34/285 [00:01<00:10, 23.81it/s]Node 193eb8ce-f909-4930-abd8-35f1447139a1 does not have a summary. Skipping filtering.\n",
      "Node a09bc1b4-a7ec-4520-ab3d-c7afdc6547ff does not have a summary. Skipping filtering.\n",
      "Node 600c3a58-61c5-4448-8a62-4210a96e4859 does not have a summary. Skipping filtering.\n",
      "Node 5efdb901-3f99-44e9-b1a2-5cd2d11be4d0 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  15%|█▌        | 43/285 [00:01<00:07, 32.06it/s]Node 8a4e80c5-e53b-4f78-8e52-8a45852e2eb5 does not have a summary. Skipping filtering.\n",
      "Node 04498385-990c-4428-8d16-225ab295edce does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  17%|█▋        | 49/285 [00:01<00:06, 35.44it/s]Node 9e723908-9a53-4628-95eb-ba9e69a4d308 does not have a summary. Skipping filtering.\n",
      "Node 053f8511-7619-4b1a-8330-16fc6e0fb3ff does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  19%|█▉        | 55/285 [00:02<00:08, 25.83it/s]Node e9d15d36-dbc7-4eea-80c1-6333ac4818a7 does not have a summary. Skipping filtering.\n",
      "Node 50420526-ef7d-48b9-a9b8-faa2d4a94cac does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  21%|██▏       | 61/285 [00:02<00:07, 30.02it/s]Node c0294a99-4e92-4eac-be55-98b1154571cc does not have a summary. Skipping filtering.\n",
      "Node 1cc12c5d-f628-4403-b1b0-84871da859da does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  24%|██▎       | 67/285 [00:02<00:06, 32.58it/s]Node 9c492358-0e80-4081-8778-63557c01dd1d does not have a summary. Skipping filtering.\n",
      "Node b38891c0-c6e8-41cc-abe0-8b6831c89165 does not have a summary. Skipping filtering.\n",
      "Node aa94271c-eb92-470c-9add-4faf53a9ce46 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  29%|██▉       | 83/285 [00:03<00:07, 26.44it/s]Node da09f9c9-d42b-436d-8cea-41f37cc2c73d does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  32%|███▏      | 91/285 [00:03<00:07, 24.64it/s]Node 01e24747-70c5-4e94-86eb-4636ae1391cc does not have a summary. Skipping filtering.\n",
      "Node 7f279ff4-0db0-4bbd-8d90-df830534bc7f does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  34%|███▍      | 97/285 [00:03<00:07, 25.12it/s]Node 6b68cfbc-d2f6-4636-8283-be98c5e21326 does not have a summary. Skipping filtering.\n",
      "Node 0f4f1163-1d5c-4217-8519-78bded361341 does not have a summary. Skipping filtering.\n",
      "Node be800968-55cb-4ad0-b1e8-bd1d50512619 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  36%|███▌      | 103/285 [00:03<00:05, 30.45it/s]Node 1a707d8d-cac5-40be-bdd7-2c3ea5836fd7 does not have a summary. Skipping filtering.\n",
      "Node 26e5c69e-4947-4b05-aca1-86d8ebecf085 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  38%|███▊      | 107/285 [00:03<00:05, 30.34it/s]Node f4ea60c2-43ec-4b32-9e6d-123551f4d00f does not have a summary. Skipping filtering.\n",
      "Node c0b1fb64-2ca8-49ae-880e-a88e5c96d05f does not have a summary. Skipping filtering.\n",
      "Node c63d72dd-f624-4e41-b9f7-536178a45c0f does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  40%|████      | 115/285 [00:04<00:05, 33.60it/s]Node 5f6d1c2e-b5fa-4f90-803c-9afa8d91d6a5 does not have a summary. Skipping filtering.\n",
      "Node 29fbd3ec-e269-4afa-a1d5-b6ade8cb5121 does not have a summary. Skipping filtering.\n",
      "Node 2f314a63-7f48-49d4-bb0a-4ff4b2f1e744 does not have a summary. Skipping filtering.\n",
      "Node 6e9fea84-f090-4747-be42-3374e17b4337 does not have a summary. Skipping filtering.\n",
      "Node 82bfcc37-0aad-40c7-bf5a-a5c67885720d does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  42%|████▏     | 119/285 [00:05<00:19,  8.30it/s]Node f6e5cd9c-b334-4eb7-96bd-de6caecbfd1a does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  48%|████▊     | 137/285 [00:06<00:10, 14.21it/s]Node 747272af-2d1b-4e36-8fdd-00d958096769 does not have a summary. Skipping filtering.\n",
      "Node 7e433d0c-9fb5-475d-86a2-7d0aa70c12a5 does not have a summary. Skipping filtering.\n",
      "Node 061cd12a-21f4-4323-90d4-dcbcc7fb28ef does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  54%|█████▍    | 155/285 [00:07<00:07, 16.33it/s]Node 3625fd04-6cf2-45a5-99c0-259d00ccdd8b does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  60%|██████    | 171/285 [00:07<00:06, 18.53it/s]Node 63b45136-40b4-45a2-aacb-137950eb51a7 does not have a summary. Skipping filtering.\n",
      "Node 70156b74-6ab9-4cfa-b6a6-ad87243fd06d does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  61%|██████    | 174/285 [00:08<00:05, 19.24it/s]Node b2722b77-c2b3-42a4-8d10-69451b763b39 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  64%|██████▍   | 182/285 [00:08<00:03, 28.02it/s]Node 1b5c6301-13f3-4b10-838f-5f873a305931 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  66%|██████▌   | 187/285 [00:08<00:03, 28.58it/s]Node bd35d30f-7eed-4668-8e0c-77cffd99ec4e does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  67%|██████▋   | 191/285 [00:08<00:04, 20.11it/s]Node 3009c9a5-3459-4e20-b3f7-1e7576c549aa does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  68%|██████▊   | 195/285 [00:08<00:04, 20.95it/s]Node 250e44ae-06cc-48c0-8dc7-a94786732c25 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  71%|███████   | 201/285 [00:08<00:03, 26.02it/s]Node 89917ec8-8c75-4abe-b49a-7b10d3dbceb1 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  76%|███████▋  | 218/285 [00:09<00:02, 23.24it/s]Node 4d349cd2-0a87-4c91-b3ed-eef70f4df58e does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  81%|████████  | 231/285 [00:10<00:02, 23.01it/s]Node e23a4f08-e9df-4578-8935-80d1267badef does not have a summary. Skipping filtering.\n",
      "Node e2f5900a-be13-489a-9be6-b20974a20ca8 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  85%|████████▍ | 241/285 [00:10<00:02, 21.37it/s]Node 4acd3616-e2df-4da9-bcda-81b9461f06dd does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  87%|████████▋ | 247/285 [00:10<00:01, 26.86it/s]Node 8dd8e454-7202-42e8-80ab-fd2f85c25c2a does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  88%|████████▊ | 250/285 [00:11<00:01, 21.75it/s]Node bb72f3e4-52f7-4170-aaa7-2eb256df6022 does not have a summary. Skipping filtering.\n",
      "Node baeaadac-9130-4adf-9e42-9d2de4e64e82 does not have a summary. Skipping filtering.\n",
      "Node 0befabe9-31e9-4538-b926-e6a31a0d6fb4 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  91%|█████████ | 259/285 [00:11<00:01, 25.29it/s]Node 54984795-82e6-47ce-8330-a57a839317bf does not have a summary. Skipping filtering.\n",
      "Node 53b47958-6a7a-4ea3-85f7-b0fe6054abf7 does not have a summary. Skipping filtering.\n",
      "Node da11cbf1-4986-4626-a5cc-52979c2fe08b does not have a summary. Skipping filtering.\n",
      "Node 802faeb6-bc6a-4de7-83d9-f8a04fb4107b does not have a summary. Skipping filtering.\n",
      "Generating Scenarios: 100%|██████████| 3/3 [21:35<00:00, 431.92s/it]                                            \n",
      "Generating Samples: 100%|██████████| 21/21 [00:09<00:00,  2.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "# 합성 데이터 생성\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings, persona_list=personas)\n",
    "dataset = generator.generate_with_langchain_docs(final_chunks, testset_size=20) # 47m 6.8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00b3266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "test_data = dataset.to_pandas()\n",
    "test_data.to_excel(\"data/test_data.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa58b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is Niki Parmar in the context of the Trans...</td>\n",
       "      <td>['Provided proper attribution is provided, Goo...</td>\n",
       "      <td>Niki Parmar is listed as an author of the pape...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How is the Transformer model applied to Englis...</td>\n",
       "      <td>['s\\nentirely. Experiments on two machine tran...</td>\n",
       "      <td>The Transformer model is applied successfully ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Whaat is a Tranformer and whoo were the main p...</td>\n",
       "      <td>['∗Equal contribution. Listing order is random...</td>\n",
       "      <td>The Transformer was first designed and impleme...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are gated recurrent neural networks used ...</td>\n",
       "      <td>['1 Introduction\\nRecurrent neural networks, l...</td>\n",
       "      <td>Gated recurrent neural networks have been firm...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what p100 gpus do in transformer training? i d...</td>\n",
       "      <td>['Recurrent models typically factor computatio...</td>\n",
       "      <td>The Transformer model can be trained to reach ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Who is Niki Parmar in the context of the Trans...   \n",
       "1  How is the Transformer model applied to Englis...   \n",
       "2  Whaat is a Tranformer and whoo were the main p...   \n",
       "3  What are gated recurrent neural networks used ...   \n",
       "4  what p100 gpus do in transformer training? i d...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  ['Provided proper attribution is provided, Goo...   \n",
       "1  ['s\\nentirely. Experiments on two machine tran...   \n",
       "2  ['∗Equal contribution. Listing order is random...   \n",
       "3  ['1 Introduction\\nRecurrent neural networks, l...   \n",
       "4  ['Recurrent models typically factor computatio...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Niki Parmar is listed as an author of the pape...   \n",
       "1  The Transformer model is applied successfully ...   \n",
       "2  The Transformer was first designed and impleme...   \n",
       "3  Gated recurrent neural networks have been firm...   \n",
       "4  The Transformer model can be trained to reach ...   \n",
       "\n",
       "                       synthesizer_name  \n",
       "0  single_hop_specifc_query_synthesizer  \n",
       "1  single_hop_specifc_query_synthesizer  \n",
       "2  single_hop_specifc_query_synthesizer  \n",
       "3  single_hop_specifc_query_synthesizer  \n",
       "4  single_hop_specifc_query_synthesizer  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 로드\n",
    "test_data = pd.read_excel(\"data/test_data.xlsx\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22cf9370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 벡터스토어 로드\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    dimensions=1024\n",
    ")\n",
    "chroma_db = Chroma(\n",
    "    collection_name=\"papers\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")\n",
    "\n",
    "# 벡터 검색기 생성\n",
    "retriever = chroma_db.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6084b40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is Niki Parmar in the context of the Trans...</td>\n",
       "      <td>['Provided proper attribution is provided, Goo...</td>\n",
       "      <td>Niki Parmar is listed as an author of the pape...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "      <td>Niki Parmar is one of the contributors to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How is the Transformer model applied to Englis...</td>\n",
       "      <td>['s\\nentirely. Experiments on two machine tran...</td>\n",
       "      <td>The Transformer model is applied successfully ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[Table 4: The Transformer generalizes well to ...</td>\n",
       "      <td>The Transformer model is applied to English co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Whaat is a Tranformer and whoo were the main p...</td>\n",
       "      <td>['∗Equal contribution. Listing order is random...</td>\n",
       "      <td>The Transformer was first designed and impleme...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[d\\nlanguage modeling tasks [34]. To the best ...</td>\n",
       "      <td>The Transformer is a neural network architectu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are gated recurrent neural networks used ...</td>\n",
       "      <td>['1 Introduction\\nRecurrent neural networks, l...</td>\n",
       "      <td>Gated recurrent neural networks have been firm...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[1 Introduction\\nRecurrent neural networks, lo...</td>\n",
       "      <td>Gated recurrent neural networks are used as st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what p100 gpus do in transformer training? i d...</td>\n",
       "      <td>['Recurrent models typically factor computatio...</td>\n",
       "      <td>The Transformer model can be trained to reach ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[. The\\ntraining objective is to reconstruct ¯...</td>\n",
       "      <td>Based on the provided context, the role of GPU...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Who is Niki Parmar in the context of the Trans...   \n",
       "1  How is the Transformer model applied to Englis...   \n",
       "2  Whaat is a Tranformer and whoo were the main p...   \n",
       "3  What are gated recurrent neural networks used ...   \n",
       "4  what p100 gpus do in transformer training? i d...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  ['Provided proper attribution is provided, Goo...   \n",
       "1  ['s\\nentirely. Experiments on two machine tran...   \n",
       "2  ['∗Equal contribution. Listing order is random...   \n",
       "3  ['1 Introduction\\nRecurrent neural networks, l...   \n",
       "4  ['Recurrent models typically factor computatio...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Niki Parmar is listed as an author of the pape...   \n",
       "1  The Transformer model is applied successfully ...   \n",
       "2  The Transformer was first designed and impleme...   \n",
       "3  Gated recurrent neural networks have been firm...   \n",
       "4  The Transformer model can be trained to reach ...   \n",
       "\n",
       "                       synthesizer_name  \\\n",
       "0  single_hop_specifc_query_synthesizer   \n",
       "1  single_hop_specifc_query_synthesizer   \n",
       "2  single_hop_specifc_query_synthesizer   \n",
       "3  single_hop_specifc_query_synthesizer   \n",
       "4  single_hop_specifc_query_synthesizer   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [∗Equal contribution. Listing order is random....   \n",
       "1  [Table 4: The Transformer generalizes well to ...   \n",
       "2  [d\\nlanguage modeling tasks [34]. To the best ...   \n",
       "3  [1 Introduction\\nRecurrent neural networks, lo...   \n",
       "4  [. The\\ntraining objective is to reconstruct ¯...   \n",
       "\n",
       "                                            response  \n",
       "0  Niki Parmar is one of the contributors to the ...  \n",
       "1  The Transformer model is applied to English co...  \n",
       "2  The Transformer is a neural network architectu...  \n",
       "3  Gated recurrent neural networks are used as st...  \n",
       "4  Based on the provided context, the role of GPU...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 검색된 문서\n",
    "test_data['retrieved_contexts'] = test_data.user_input.apply(\n",
    "    lambda query: [doc.page_content for doc in retriever.invoke(query)]\n",
    ")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f4468e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is Niki Parmar in the context of the Trans...</td>\n",
       "      <td>['Provided proper attribution is provided, Goo...</td>\n",
       "      <td>Niki Parmar is listed as an author of the pape...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "      <td>Niki Parmar is one of the contributors to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How is the Transformer model applied to Englis...</td>\n",
       "      <td>['s\\nentirely. Experiments on two machine tran...</td>\n",
       "      <td>The Transformer model is applied successfully ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[Table 4: The Transformer generalizes well to ...</td>\n",
       "      <td>The Transformer model is applied to English co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Whaat is a Tranformer and whoo were the main p...</td>\n",
       "      <td>['∗Equal contribution. Listing order is random...</td>\n",
       "      <td>The Transformer was first designed and impleme...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[d\\nlanguage modeling tasks [34]. To the best ...</td>\n",
       "      <td>The Transformer is a neural network architectu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are gated recurrent neural networks used ...</td>\n",
       "      <td>['1 Introduction\\nRecurrent neural networks, l...</td>\n",
       "      <td>Gated recurrent neural networks have been firm...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[1 Introduction\\nRecurrent neural networks, lo...</td>\n",
       "      <td>Gated recurrent neural networks are used as st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what p100 gpus do in transformer training? i d...</td>\n",
       "      <td>['Recurrent models typically factor computatio...</td>\n",
       "      <td>The Transformer model can be trained to reach ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[. The\\ntraining objective is to reconstruct ¯...</td>\n",
       "      <td>Based on the provided context, the role of GPU...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Who is Niki Parmar in the context of the Trans...   \n",
       "1  How is the Transformer model applied to Englis...   \n",
       "2  Whaat is a Tranformer and whoo were the main p...   \n",
       "3  What are gated recurrent neural networks used ...   \n",
       "4  what p100 gpus do in transformer training? i d...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  ['Provided proper attribution is provided, Goo...   \n",
       "1  ['s\\nentirely. Experiments on two machine tran...   \n",
       "2  ['∗Equal contribution. Listing order is random...   \n",
       "3  ['1 Introduction\\nRecurrent neural networks, l...   \n",
       "4  ['Recurrent models typically factor computatio...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Niki Parmar is listed as an author of the pape...   \n",
       "1  The Transformer model is applied successfully ...   \n",
       "2  The Transformer was first designed and impleme...   \n",
       "3  Gated recurrent neural networks have been firm...   \n",
       "4  The Transformer model can be trained to reach ...   \n",
       "\n",
       "                       synthesizer_name  \\\n",
       "0  single_hop_specifc_query_synthesizer   \n",
       "1  single_hop_specifc_query_synthesizer   \n",
       "2  single_hop_specifc_query_synthesizer   \n",
       "3  single_hop_specifc_query_synthesizer   \n",
       "4  single_hop_specifc_query_synthesizer   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [∗Equal contribution. Listing order is random....   \n",
       "1  [Table 4: The Transformer generalizes well to ...   \n",
       "2  [d\\nlanguage modeling tasks [34]. To the best ...   \n",
       "3  [1 Introduction\\nRecurrent neural networks, lo...   \n",
       "4  [. The\\ntraining objective is to reconstruct ¯...   \n",
       "\n",
       "                                            response  \n",
       "0  Niki Parmar is one of the contributors to the ...  \n",
       "1  The Transformer model is applied to English co...  \n",
       "2  The Transformer is a neural network architectu...  \n",
       "3  Gated recurrent neural networks are used as st...  \n",
       "4  Based on the provided context, the role of GPU...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 템플릿 생성\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "[Context]\n",
    "{context}\n",
    "\n",
    "[Question]\n",
    "{query}\n",
    "\n",
    "[Answer]\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# RAG 체인 구성\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "qa_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# RAG 체인 생성 답변\n",
    "test_data['response'] = test_data.apply(\n",
    "    lambda row: qa_chain.invoke({\n",
    "        \"context\": \"\\n\".join(row.retrieved_contexts), \n",
    "        \"query\": row.user_input\n",
    "    }),\n",
    "    axis=1\n",
    ")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb20b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 63/63 [01:07<00:00,  1.07s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 0.5660, 'faithfulness': 0.7819, 'factual_correctness(mode=f1)': 0.3619}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import EvaluationDataset\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness\n",
    "\n",
    "# 평가셋 처리\n",
    "evaluation_dataset = EvaluationDataset.from_pandas(\n",
    "    test_data[['user_input', 'retrieved_contexts', 'response', 'reference']]\n",
    ")\n",
    "\n",
    "# LLM 래퍼 생성\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1\", temperature=0))\n",
    "\n",
    "# 생성 평가 (기본 베이스 라인)\n",
    "result = evaluate(\n",
    "    dataset=evaluation_dataset,   # 평가 데이터셋\n",
    "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness()],   # 평가 메트릭\n",
    "    llm=evaluator_llm,   # LLM 래퍼\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30bf27c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 결과 저장\n",
    "result.to_pandas().to_excel('data/evaluation_dataset.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408549c3",
   "metadata": {},
   "source": [
    "### 2\\) Retrieval Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a673882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "# Semantic 검색기 생성\n",
    "chroma_k_retriever = chroma_db.as_retriever(\n",
    "    search_kwargs={\"k\": 5},\n",
    ")\n",
    "\n",
    "# BM25 검색기 생성\n",
    "bm25_retriever = BM25Retriever.from_documents(final_chunks)\n",
    "\n",
    "# 앙상블 검색기 생성\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[chroma_k_retriever, bm25_retriever], \n",
    "    weights=[0.7, 0.3]          # 각 검색기의 가중치\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "983a49ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>hyde_query</th>\n",
       "      <th>bm25_contexts</th>\n",
       "      <th>ensemble_contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is Niki Parmar in the context of the Trans...</td>\n",
       "      <td>['Provided proper attribution is provided, Goo...</td>\n",
       "      <td>Niki Parmar is listed as an author of the pape...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "      <td>Niki Parmar is one of the contributors to the ...</td>\n",
       "      <td>Niki Parmar is one of the co-authors of the se...</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How is the Transformer model applied to Englis...</td>\n",
       "      <td>['s\\nentirely. Experiments on two machine tran...</td>\n",
       "      <td>The Transformer model is applied successfully ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[Table 4: The Transformer generalizes well to ...</td>\n",
       "      <td>The Transformer model is applied to English co...</td>\n",
       "      <td>**Application of the Transformer Model to Engl...</td>\n",
       "      <td>[Table 4: The Transformer generalizes well to ...</td>\n",
       "      <td>[Table 4: The Transformer generalizes well to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Whaat is a Tranformer and whoo were the main p...</td>\n",
       "      <td>['∗Equal contribution. Listing order is random...</td>\n",
       "      <td>The Transformer was first designed and impleme...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[d\\nlanguage modeling tasks [34]. To the best ...</td>\n",
       "      <td>The Transformer is a neural network architectu...</td>\n",
       "      <td>**Transformer: Definition and Key Contributors...</td>\n",
       "      <td>[n\\nimprovement in our setting. Hence, we excl...</td>\n",
       "      <td>[d\\nlanguage modeling tasks [34]. To the best ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are gated recurrent neural networks used ...</td>\n",
       "      <td>['1 Introduction\\nRecurrent neural networks, l...</td>\n",
       "      <td>Gated recurrent neural networks have been firm...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[1 Introduction\\nRecurrent neural networks, lo...</td>\n",
       "      <td>Gated recurrent neural networks are used as st...</td>\n",
       "      <td>**Gated Recurrent Neural Networks in Sequence ...</td>\n",
       "      <td>[1 Introduction\\nRecurrent neural networks, lo...</td>\n",
       "      <td>[1 Introduction\\nRecurrent neural networks, lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what p100 gpus do in transformer training? i d...</td>\n",
       "      <td>['Recurrent models typically factor computatio...</td>\n",
       "      <td>The Transformer model can be trained to reach ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[. The\\ntraining objective is to reconstruct ¯...</td>\n",
       "      <td>Based on the provided context, the role of GPU...</td>\n",
       "      <td>**The Role of NVIDIA P100 GPUs in Transformer ...</td>\n",
       "      <td>[output values. These are concatenated and onc...</td>\n",
       "      <td>[. The\\ntraining objective is to reconstruct ¯...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Who is Niki Parmar in the context of the Trans...   \n",
       "1  How is the Transformer model applied to Englis...   \n",
       "2  Whaat is a Tranformer and whoo were the main p...   \n",
       "3  What are gated recurrent neural networks used ...   \n",
       "4  what p100 gpus do in transformer training? i d...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  ['Provided proper attribution is provided, Goo...   \n",
       "1  ['s\\nentirely. Experiments on two machine tran...   \n",
       "2  ['∗Equal contribution. Listing order is random...   \n",
       "3  ['1 Introduction\\nRecurrent neural networks, l...   \n",
       "4  ['Recurrent models typically factor computatio...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Niki Parmar is listed as an author of the pape...   \n",
       "1  The Transformer model is applied successfully ...   \n",
       "2  The Transformer was first designed and impleme...   \n",
       "3  Gated recurrent neural networks have been firm...   \n",
       "4  The Transformer model can be trained to reach ...   \n",
       "\n",
       "                       synthesizer_name  \\\n",
       "0  single_hop_specifc_query_synthesizer   \n",
       "1  single_hop_specifc_query_synthesizer   \n",
       "2  single_hop_specifc_query_synthesizer   \n",
       "3  single_hop_specifc_query_synthesizer   \n",
       "4  single_hop_specifc_query_synthesizer   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [∗Equal contribution. Listing order is random....   \n",
       "1  [Table 4: The Transformer generalizes well to ...   \n",
       "2  [d\\nlanguage modeling tasks [34]. To the best ...   \n",
       "3  [1 Introduction\\nRecurrent neural networks, lo...   \n",
       "4  [. The\\ntraining objective is to reconstruct ¯...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Niki Parmar is one of the contributors to the ...   \n",
       "1  The Transformer model is applied to English co...   \n",
       "2  The Transformer is a neural network architectu...   \n",
       "3  Gated recurrent neural networks are used as st...   \n",
       "4  Based on the provided context, the role of GPU...   \n",
       "\n",
       "                                          hyde_query  \\\n",
       "0  Niki Parmar is one of the co-authors of the se...   \n",
       "1  **Application of the Transformer Model to Engl...   \n",
       "2  **Transformer: Definition and Key Contributors...   \n",
       "3  **Gated Recurrent Neural Networks in Sequence ...   \n",
       "4  **The Role of NVIDIA P100 GPUs in Transformer ...   \n",
       "\n",
       "                                       bm25_contexts  \\\n",
       "0  [∗Equal contribution. Listing order is random....   \n",
       "1  [Table 4: The Transformer generalizes well to ...   \n",
       "2  [n\\nimprovement in our setting. Hence, we excl...   \n",
       "3  [1 Introduction\\nRecurrent neural networks, lo...   \n",
       "4  [output values. These are concatenated and onc...   \n",
       "\n",
       "                                   ensemble_contexts  \n",
       "0  [∗Equal contribution. Listing order is random....  \n",
       "1  [Table 4: The Transformer generalizes well to ...  \n",
       "2  [d\\nlanguage modeling tasks [34]. To the best ...  \n",
       "3  [1 Introduction\\nRecurrent neural networks, lo...  \n",
       "4  [. The\\ntraining objective is to reconstruct ¯...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 검색기 실행\n",
    "test_data['bm25_contexts'] = test_data.user_input.apply(\n",
    "    lambda query: [doc.page_content for doc in bm25_retriever.invoke(query)]\n",
    ")\n",
    "test_data['ensemble_contexts'] = test_data.user_input.apply(\n",
    "    lambda query: [doc.page_content for doc in ensemble_retriever.invoke(query)]\n",
    ")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f775cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 문서 생성 체인 생성 (HyDE 기법)\n",
    "template = \"\"\"Please create the ideal document content for the given question.\n",
    "The document should be written in an academic and professional tone.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Document content:\"\"\"\n",
    "hyde_prompt = ChatPromptTemplate.from_template(template)\n",
    "hyde_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "hyde_chain = hyde_prompt | hyde_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae03461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>hyde_query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is Niki Parmar in the context of the Trans...</td>\n",
       "      <td>['Provided proper attribution is provided, Goo...</td>\n",
       "      <td>Niki Parmar is listed as an author of the pape...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "      <td>Niki Parmar is one of the contributors to the ...</td>\n",
       "      <td>Niki Parmar is one of the co-authors of the se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How is the Transformer model applied to Englis...</td>\n",
       "      <td>['s\\nentirely. Experiments on two machine tran...</td>\n",
       "      <td>The Transformer model is applied successfully ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[Table 4: The Transformer generalizes well to ...</td>\n",
       "      <td>The Transformer model is applied to English co...</td>\n",
       "      <td>**Application of the Transformer Model to Engl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Whaat is a Tranformer and whoo were the main p...</td>\n",
       "      <td>['∗Equal contribution. Listing order is random...</td>\n",
       "      <td>The Transformer was first designed and impleme...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[d\\nlanguage modeling tasks [34]. To the best ...</td>\n",
       "      <td>The Transformer is a neural network architectu...</td>\n",
       "      <td>**Transformer: Definition and Key Contributors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are gated recurrent neural networks used ...</td>\n",
       "      <td>['1 Introduction\\nRecurrent neural networks, l...</td>\n",
       "      <td>Gated recurrent neural networks have been firm...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[1 Introduction\\nRecurrent neural networks, lo...</td>\n",
       "      <td>Gated recurrent neural networks are used as st...</td>\n",
       "      <td>**Gated Recurrent Neural Networks in Sequence ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what p100 gpus do in transformer training? i d...</td>\n",
       "      <td>['Recurrent models typically factor computatio...</td>\n",
       "      <td>The Transformer model can be trained to reach ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[. The\\ntraining objective is to reconstruct ¯...</td>\n",
       "      <td>Based on the provided context, the role of GPU...</td>\n",
       "      <td>**The Role of NVIDIA P100 GPUs in Transformer ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Who is Niki Parmar in the context of the Trans...   \n",
       "1  How is the Transformer model applied to Englis...   \n",
       "2  Whaat is a Tranformer and whoo were the main p...   \n",
       "3  What are gated recurrent neural networks used ...   \n",
       "4  what p100 gpus do in transformer training? i d...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  ['Provided proper attribution is provided, Goo...   \n",
       "1  ['s\\nentirely. Experiments on two machine tran...   \n",
       "2  ['∗Equal contribution. Listing order is random...   \n",
       "3  ['1 Introduction\\nRecurrent neural networks, l...   \n",
       "4  ['Recurrent models typically factor computatio...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Niki Parmar is listed as an author of the pape...   \n",
       "1  The Transformer model is applied successfully ...   \n",
       "2  The Transformer was first designed and impleme...   \n",
       "3  Gated recurrent neural networks have been firm...   \n",
       "4  The Transformer model can be trained to reach ...   \n",
       "\n",
       "                       synthesizer_name  \\\n",
       "0  single_hop_specifc_query_synthesizer   \n",
       "1  single_hop_specifc_query_synthesizer   \n",
       "2  single_hop_specifc_query_synthesizer   \n",
       "3  single_hop_specifc_query_synthesizer   \n",
       "4  single_hop_specifc_query_synthesizer   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [∗Equal contribution. Listing order is random....   \n",
       "1  [Table 4: The Transformer generalizes well to ...   \n",
       "2  [d\\nlanguage modeling tasks [34]. To the best ...   \n",
       "3  [1 Introduction\\nRecurrent neural networks, lo...   \n",
       "4  [. The\\ntraining objective is to reconstruct ¯...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Niki Parmar is one of the contributors to the ...   \n",
       "1  The Transformer model is applied to English co...   \n",
       "2  The Transformer is a neural network architectu...   \n",
       "3  Gated recurrent neural networks are used as st...   \n",
       "4  Based on the provided context, the role of GPU...   \n",
       "\n",
       "                                          hyde_query  \n",
       "0  Niki Parmar is one of the co-authors of the se...  \n",
       "1  **Application of the Transformer Model to Engl...  \n",
       "2  **Transformer: Definition and Key Contributors...  \n",
       "3  **Gated Recurrent Neural Networks in Sequence ...  \n",
       "4  **The Role of NVIDIA P100 GPUs in Transformer ...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HyDE 기법 적용 쿼리 생성\n",
    "test_data['hyde_query'] = test_data.user_input.apply(\n",
    "    lambda query: hyde_chain.invoke({\"question\": query})\n",
    ")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7458b70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>hyde_query</th>\n",
       "      <th>bm25_contexts</th>\n",
       "      <th>ensemble_contexts</th>\n",
       "      <th>hyde_ensemble_contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is Niki Parmar in the context of the Trans...</td>\n",
       "      <td>['Provided proper attribution is provided, Goo...</td>\n",
       "      <td>Niki Parmar is listed as an author of the pape...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "      <td>Niki Parmar is one of the contributors to the ...</td>\n",
       "      <td>Niki Parmar is one of the co-authors of the se...</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How is the Transformer model applied to Englis...</td>\n",
       "      <td>['s\\nentirely. Experiments on two machine tran...</td>\n",
       "      <td>The Transformer model is applied successfully ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[Table 4: The Transformer generalizes well to ...</td>\n",
       "      <td>The Transformer model is applied to English co...</td>\n",
       "      <td>**Application of the Transformer Model to Engl...</td>\n",
       "      <td>[Table 4: The Transformer generalizes well to ...</td>\n",
       "      <td>[Table 4: The Transformer generalizes well to ...</td>\n",
       "      <td>[,\\nbigger models are better, and dropout is v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Whaat is a Tranformer and whoo were the main p...</td>\n",
       "      <td>['∗Equal contribution. Listing order is random...</td>\n",
       "      <td>The Transformer was first designed and impleme...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[d\\nlanguage modeling tasks [34]. To the best ...</td>\n",
       "      <td>The Transformer is a neural network architectu...</td>\n",
       "      <td>**Transformer: Definition and Key Contributors...</td>\n",
       "      <td>[n\\nimprovement in our setting. Hence, we excl...</td>\n",
       "      <td>[d\\nlanguage modeling tasks [34]. To the best ...</td>\n",
       "      <td>[Provided proper attribution is provided, Goog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are gated recurrent neural networks used ...</td>\n",
       "      <td>['1 Introduction\\nRecurrent neural networks, l...</td>\n",
       "      <td>Gated recurrent neural networks have been firm...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[1 Introduction\\nRecurrent neural networks, lo...</td>\n",
       "      <td>Gated recurrent neural networks are used as st...</td>\n",
       "      <td>**Gated Recurrent Neural Networks in Sequence ...</td>\n",
       "      <td>[1 Introduction\\nRecurrent neural networks, lo...</td>\n",
       "      <td>[1 Introduction\\nRecurrent neural networks, lo...</td>\n",
       "      <td>[Recurrent models typically factor computation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what p100 gpus do in transformer training? i d...</td>\n",
       "      <td>['Recurrent models typically factor computatio...</td>\n",
       "      <td>The Transformer model can be trained to reach ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[. The\\ntraining objective is to reconstruct ¯...</td>\n",
       "      <td>Based on the provided context, the role of GPU...</td>\n",
       "      <td>**The Role of NVIDIA P100 GPUs in Transformer ...</td>\n",
       "      <td>[output values. These are concatenated and onc...</td>\n",
       "      <td>[. The\\ntraining objective is to reconstruct ¯...</td>\n",
       "      <td>[Recurrent models typically factor computation...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Who is Niki Parmar in the context of the Trans...   \n",
       "1  How is the Transformer model applied to Englis...   \n",
       "2  Whaat is a Tranformer and whoo were the main p...   \n",
       "3  What are gated recurrent neural networks used ...   \n",
       "4  what p100 gpus do in transformer training? i d...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  ['Provided proper attribution is provided, Goo...   \n",
       "1  ['s\\nentirely. Experiments on two machine tran...   \n",
       "2  ['∗Equal contribution. Listing order is random...   \n",
       "3  ['1 Introduction\\nRecurrent neural networks, l...   \n",
       "4  ['Recurrent models typically factor computatio...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Niki Parmar is listed as an author of the pape...   \n",
       "1  The Transformer model is applied successfully ...   \n",
       "2  The Transformer was first designed and impleme...   \n",
       "3  Gated recurrent neural networks have been firm...   \n",
       "4  The Transformer model can be trained to reach ...   \n",
       "\n",
       "                       synthesizer_name  \\\n",
       "0  single_hop_specifc_query_synthesizer   \n",
       "1  single_hop_specifc_query_synthesizer   \n",
       "2  single_hop_specifc_query_synthesizer   \n",
       "3  single_hop_specifc_query_synthesizer   \n",
       "4  single_hop_specifc_query_synthesizer   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [∗Equal contribution. Listing order is random....   \n",
       "1  [Table 4: The Transformer generalizes well to ...   \n",
       "2  [d\\nlanguage modeling tasks [34]. To the best ...   \n",
       "3  [1 Introduction\\nRecurrent neural networks, lo...   \n",
       "4  [. The\\ntraining objective is to reconstruct ¯...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Niki Parmar is one of the contributors to the ...   \n",
       "1  The Transformer model is applied to English co...   \n",
       "2  The Transformer is a neural network architectu...   \n",
       "3  Gated recurrent neural networks are used as st...   \n",
       "4  Based on the provided context, the role of GPU...   \n",
       "\n",
       "                                          hyde_query  \\\n",
       "0  Niki Parmar is one of the co-authors of the se...   \n",
       "1  **Application of the Transformer Model to Engl...   \n",
       "2  **Transformer: Definition and Key Contributors...   \n",
       "3  **Gated Recurrent Neural Networks in Sequence ...   \n",
       "4  **The Role of NVIDIA P100 GPUs in Transformer ...   \n",
       "\n",
       "                                       bm25_contexts  \\\n",
       "0  [∗Equal contribution. Listing order is random....   \n",
       "1  [Table 4: The Transformer generalizes well to ...   \n",
       "2  [n\\nimprovement in our setting. Hence, we excl...   \n",
       "3  [1 Introduction\\nRecurrent neural networks, lo...   \n",
       "4  [output values. These are concatenated and onc...   \n",
       "\n",
       "                                   ensemble_contexts  \\\n",
       "0  [∗Equal contribution. Listing order is random....   \n",
       "1  [Table 4: The Transformer generalizes well to ...   \n",
       "2  [d\\nlanguage modeling tasks [34]. To the best ...   \n",
       "3  [1 Introduction\\nRecurrent neural networks, lo...   \n",
       "4  [. The\\ntraining objective is to reconstruct ¯...   \n",
       "\n",
       "                              hyde_ensemble_contexts  \n",
       "0  [∗Equal contribution. Listing order is random....  \n",
       "1  [,\\nbigger models are better, and dropout is v...  \n",
       "2  [Provided proper attribution is provided, Goog...  \n",
       "3  [Recurrent models typically factor computation...  \n",
       "4  [Recurrent models typically factor computation...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HyDE 기법 적용 쿼리 + 앙상블 검색\n",
    "test_data['hyde_ensemble_contexts'] = test_data.hyde_query.apply(\n",
    "    lambda query: [doc.page_content for doc in ensemble_retriever.invoke(query)]\n",
    ")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "94e25b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>hyde_query</th>\n",
       "      <th>bm25_contexts</th>\n",
       "      <th>ensemble_contexts</th>\n",
       "      <th>hyde_ensemble_contexts</th>\n",
       "      <th>total_mix_contexts</th>\n",
       "      <th>ensemble_reranker_contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is Niki Parmar in the context of the Trans...</td>\n",
       "      <td>['Provided proper attribution is provided, Goo...</td>\n",
       "      <td>Niki Parmar is listed as an author of the pape...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "      <td>Niki Parmar is one of the contributors to the ...</td>\n",
       "      <td>Niki Parmar is one of the co-authors of the se...</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "      <td>[Provided proper attribution is provided, Goog...</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How is the Transformer model applied to Englis...</td>\n",
       "      <td>['s\\nentirely. Experiments on two machine tran...</td>\n",
       "      <td>The Transformer model is applied successfully ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[Table 4: The Transformer generalizes well to ...</td>\n",
       "      <td>The Transformer model is applied to English co...</td>\n",
       "      <td>**Application of the Transformer Model to Engl...</td>\n",
       "      <td>[Table 4: The Transformer generalizes well to ...</td>\n",
       "      <td>[Table 4: The Transformer generalizes well to ...</td>\n",
       "      <td>[,\\nbigger models are better, and dropout is v...</td>\n",
       "      <td>[Table 4: The Transformer generalizes well to ...</td>\n",
       "      <td>[s\\nentirely. Experiments on two machine trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Whaat is a Tranformer and whoo were the main p...</td>\n",
       "      <td>['∗Equal contribution. Listing order is random...</td>\n",
       "      <td>The Transformer was first designed and impleme...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[d\\nlanguage modeling tasks [34]. To the best ...</td>\n",
       "      <td>The Transformer is a neural network architectu...</td>\n",
       "      <td>**Transformer: Definition and Key Contributors...</td>\n",
       "      <td>[n\\nimprovement in our setting. Hence, we excl...</td>\n",
       "      <td>[d\\nlanguage modeling tasks [34]. To the best ...</td>\n",
       "      <td>[Provided proper attribution is provided, Goog...</td>\n",
       "      <td>[Provided proper attribution is provided, Goog...</td>\n",
       "      <td>[d\\nlanguage modeling tasks [34]. To the best ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are gated recurrent neural networks used ...</td>\n",
       "      <td>['1 Introduction\\nRecurrent neural networks, l...</td>\n",
       "      <td>Gated recurrent neural networks have been firm...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[1 Introduction\\nRecurrent neural networks, lo...</td>\n",
       "      <td>Gated recurrent neural networks are used as st...</td>\n",
       "      <td>**Gated Recurrent Neural Networks in Sequence ...</td>\n",
       "      <td>[1 Introduction\\nRecurrent neural networks, lo...</td>\n",
       "      <td>[1 Introduction\\nRecurrent neural networks, lo...</td>\n",
       "      <td>[Recurrent models typically factor computation...</td>\n",
       "      <td>[1 Introduction\\nRecurrent neural networks, lo...</td>\n",
       "      <td>[1 Introduction\\nRecurrent neural networks, lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what p100 gpus do in transformer training? i d...</td>\n",
       "      <td>['Recurrent models typically factor computatio...</td>\n",
       "      <td>The Transformer model can be trained to reach ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[. The\\ntraining objective is to reconstruct ¯...</td>\n",
       "      <td>Based on the provided context, the role of GPU...</td>\n",
       "      <td>**The Role of NVIDIA P100 GPUs in Transformer ...</td>\n",
       "      <td>[output values. These are concatenated and onc...</td>\n",
       "      <td>[. The\\ntraining objective is to reconstruct ¯...</td>\n",
       "      <td>[Recurrent models typically factor computation...</td>\n",
       "      <td>[In addition, we apply dropout to the sums of ...</td>\n",
       "      <td>[e\\nprevious state-of-the-art model. The Trans...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Who is Niki Parmar in the context of the Trans...   \n",
       "1  How is the Transformer model applied to Englis...   \n",
       "2  Whaat is a Tranformer and whoo were the main p...   \n",
       "3  What are gated recurrent neural networks used ...   \n",
       "4  what p100 gpus do in transformer training? i d...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  ['Provided proper attribution is provided, Goo...   \n",
       "1  ['s\\nentirely. Experiments on two machine tran...   \n",
       "2  ['∗Equal contribution. Listing order is random...   \n",
       "3  ['1 Introduction\\nRecurrent neural networks, l...   \n",
       "4  ['Recurrent models typically factor computatio...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Niki Parmar is listed as an author of the pape...   \n",
       "1  The Transformer model is applied successfully ...   \n",
       "2  The Transformer was first designed and impleme...   \n",
       "3  Gated recurrent neural networks have been firm...   \n",
       "4  The Transformer model can be trained to reach ...   \n",
       "\n",
       "                       synthesizer_name  \\\n",
       "0  single_hop_specifc_query_synthesizer   \n",
       "1  single_hop_specifc_query_synthesizer   \n",
       "2  single_hop_specifc_query_synthesizer   \n",
       "3  single_hop_specifc_query_synthesizer   \n",
       "4  single_hop_specifc_query_synthesizer   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [∗Equal contribution. Listing order is random....   \n",
       "1  [Table 4: The Transformer generalizes well to ...   \n",
       "2  [d\\nlanguage modeling tasks [34]. To the best ...   \n",
       "3  [1 Introduction\\nRecurrent neural networks, lo...   \n",
       "4  [. The\\ntraining objective is to reconstruct ¯...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Niki Parmar is one of the contributors to the ...   \n",
       "1  The Transformer model is applied to English co...   \n",
       "2  The Transformer is a neural network architectu...   \n",
       "3  Gated recurrent neural networks are used as st...   \n",
       "4  Based on the provided context, the role of GPU...   \n",
       "\n",
       "                                          hyde_query  \\\n",
       "0  Niki Parmar is one of the co-authors of the se...   \n",
       "1  **Application of the Transformer Model to Engl...   \n",
       "2  **Transformer: Definition and Key Contributors...   \n",
       "3  **Gated Recurrent Neural Networks in Sequence ...   \n",
       "4  **The Role of NVIDIA P100 GPUs in Transformer ...   \n",
       "\n",
       "                                       bm25_contexts  \\\n",
       "0  [∗Equal contribution. Listing order is random....   \n",
       "1  [Table 4: The Transformer generalizes well to ...   \n",
       "2  [n\\nimprovement in our setting. Hence, we excl...   \n",
       "3  [1 Introduction\\nRecurrent neural networks, lo...   \n",
       "4  [output values. These are concatenated and onc...   \n",
       "\n",
       "                                   ensemble_contexts  \\\n",
       "0  [∗Equal contribution. Listing order is random....   \n",
       "1  [Table 4: The Transformer generalizes well to ...   \n",
       "2  [d\\nlanguage modeling tasks [34]. To the best ...   \n",
       "3  [1 Introduction\\nRecurrent neural networks, lo...   \n",
       "4  [. The\\ntraining objective is to reconstruct ¯...   \n",
       "\n",
       "                              hyde_ensemble_contexts  \\\n",
       "0  [∗Equal contribution. Listing order is random....   \n",
       "1  [,\\nbigger models are better, and dropout is v...   \n",
       "2  [Provided proper attribution is provided, Goog...   \n",
       "3  [Recurrent models typically factor computation...   \n",
       "4  [Recurrent models typically factor computation...   \n",
       "\n",
       "                                  total_mix_contexts  \\\n",
       "0  [Provided proper attribution is provided, Goog...   \n",
       "1  [Table 4: The Transformer generalizes well to ...   \n",
       "2  [Provided proper attribution is provided, Goog...   \n",
       "3  [1 Introduction\\nRecurrent neural networks, lo...   \n",
       "4  [In addition, we apply dropout to the sums of ...   \n",
       "\n",
       "                          ensemble_reranker_contexts  \n",
       "0  [∗Equal contribution. Listing order is random....  \n",
       "1  [s\\nentirely. Experiments on two machine trans...  \n",
       "2  [d\\nlanguage modeling tasks [34]. To the best ...  \n",
       "3  [1 Introduction\\nRecurrent neural networks, lo...  \n",
       "4  [e\\nprevious state-of-the-art model. The Trans...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "# CrossEncoderReranker 모델 초기화 \n",
    "model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\")\n",
    "re_ranker = CrossEncoderReranker(model=model, top_n=3)\n",
    "\n",
    "# 앙상블 검색기 + CrossEncoderReranker 문서 검색\n",
    "cross_encoder_reranker_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=re_ranker, \n",
    "    base_retriever=ensemble_retriever,\n",
    ")\n",
    "test_data['ensemble_reranker_contexts'] = test_data.user_input.apply(\n",
    "    lambda query: [doc.page_content for doc in cross_encoder_reranker_retriever.invoke(query)]\n",
    ")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b102301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>hyde_query</th>\n",
       "      <th>bm25_contexts</th>\n",
       "      <th>ensemble_contexts</th>\n",
       "      <th>hyde_ensemble_contexts</th>\n",
       "      <th>ensemble_cross_contexts</th>\n",
       "      <th>total_mix_contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is Niki Parmar in the context of the Trans...</td>\n",
       "      <td>['Provided proper attribution is provided, Goo...</td>\n",
       "      <td>Niki Parmar is listed as an author of the pape...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "      <td>Niki Parmar is one of the contributors to the ...</td>\n",
       "      <td>Niki Parmar is one of the co-authors of the se...</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "      <td>[Provided proper attribution is provided, Goog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How is the Transformer model applied to Englis...</td>\n",
       "      <td>['s\\nentirely. Experiments on two machine tran...</td>\n",
       "      <td>The Transformer model is applied successfully ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[Table 4: The Transformer generalizes well to ...</td>\n",
       "      <td>The Transformer model is applied to English co...</td>\n",
       "      <td>**Application of the Transformer Model to Engl...</td>\n",
       "      <td>[Table 4: The Transformer generalizes well to ...</td>\n",
       "      <td>[Table 4: The Transformer generalizes well to ...</td>\n",
       "      <td>[,\\nbigger models are better, and dropout is v...</td>\n",
       "      <td>[s\\nentirely. Experiments on two machine trans...</td>\n",
       "      <td>[Table 4: The Transformer generalizes well to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Whaat is a Tranformer and whoo were the main p...</td>\n",
       "      <td>['∗Equal contribution. Listing order is random...</td>\n",
       "      <td>The Transformer was first designed and impleme...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[d\\nlanguage modeling tasks [34]. To the best ...</td>\n",
       "      <td>The Transformer is a neural network architectu...</td>\n",
       "      <td>**Transformer: Definition and Key Contributors...</td>\n",
       "      <td>[n\\nimprovement in our setting. Hence, we excl...</td>\n",
       "      <td>[d\\nlanguage modeling tasks [34]. To the best ...</td>\n",
       "      <td>[Provided proper attribution is provided, Goog...</td>\n",
       "      <td>[d\\nlanguage modeling tasks [34]. To the best ...</td>\n",
       "      <td>[Provided proper attribution is provided, Goog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are gated recurrent neural networks used ...</td>\n",
       "      <td>['1 Introduction\\nRecurrent neural networks, l...</td>\n",
       "      <td>Gated recurrent neural networks have been firm...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[1 Introduction\\nRecurrent neural networks, lo...</td>\n",
       "      <td>Gated recurrent neural networks are used as st...</td>\n",
       "      <td>**Gated Recurrent Neural Networks in Sequence ...</td>\n",
       "      <td>[1 Introduction\\nRecurrent neural networks, lo...</td>\n",
       "      <td>[1 Introduction\\nRecurrent neural networks, lo...</td>\n",
       "      <td>[Recurrent models typically factor computation...</td>\n",
       "      <td>[1 Introduction\\nRecurrent neural networks, lo...</td>\n",
       "      <td>[1 Introduction\\nRecurrent neural networks, lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what p100 gpus do in transformer training? i d...</td>\n",
       "      <td>['Recurrent models typically factor computatio...</td>\n",
       "      <td>The Transformer model can be trained to reach ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[. The\\ntraining objective is to reconstruct ¯...</td>\n",
       "      <td>Based on the provided context, the role of GPU...</td>\n",
       "      <td>**The Role of NVIDIA P100 GPUs in Transformer ...</td>\n",
       "      <td>[output values. These are concatenated and onc...</td>\n",
       "      <td>[. The\\ntraining objective is to reconstruct ¯...</td>\n",
       "      <td>[Recurrent models typically factor computation...</td>\n",
       "      <td>[e\\nprevious state-of-the-art model. The Trans...</td>\n",
       "      <td>[In addition, we apply dropout to the sums of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Who is Niki Parmar in the context of the Trans...   \n",
       "1  How is the Transformer model applied to Englis...   \n",
       "2  Whaat is a Tranformer and whoo were the main p...   \n",
       "3  What are gated recurrent neural networks used ...   \n",
       "4  what p100 gpus do in transformer training? i d...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  ['Provided proper attribution is provided, Goo...   \n",
       "1  ['s\\nentirely. Experiments on two machine tran...   \n",
       "2  ['∗Equal contribution. Listing order is random...   \n",
       "3  ['1 Introduction\\nRecurrent neural networks, l...   \n",
       "4  ['Recurrent models typically factor computatio...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Niki Parmar is listed as an author of the pape...   \n",
       "1  The Transformer model is applied successfully ...   \n",
       "2  The Transformer was first designed and impleme...   \n",
       "3  Gated recurrent neural networks have been firm...   \n",
       "4  The Transformer model can be trained to reach ...   \n",
       "\n",
       "                       synthesizer_name  \\\n",
       "0  single_hop_specifc_query_synthesizer   \n",
       "1  single_hop_specifc_query_synthesizer   \n",
       "2  single_hop_specifc_query_synthesizer   \n",
       "3  single_hop_specifc_query_synthesizer   \n",
       "4  single_hop_specifc_query_synthesizer   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [∗Equal contribution. Listing order is random....   \n",
       "1  [Table 4: The Transformer generalizes well to ...   \n",
       "2  [d\\nlanguage modeling tasks [34]. To the best ...   \n",
       "3  [1 Introduction\\nRecurrent neural networks, lo...   \n",
       "4  [. The\\ntraining objective is to reconstruct ¯...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Niki Parmar is one of the contributors to the ...   \n",
       "1  The Transformer model is applied to English co...   \n",
       "2  The Transformer is a neural network architectu...   \n",
       "3  Gated recurrent neural networks are used as st...   \n",
       "4  Based on the provided context, the role of GPU...   \n",
       "\n",
       "                                          hyde_query  \\\n",
       "0  Niki Parmar is one of the co-authors of the se...   \n",
       "1  **Application of the Transformer Model to Engl...   \n",
       "2  **Transformer: Definition and Key Contributors...   \n",
       "3  **Gated Recurrent Neural Networks in Sequence ...   \n",
       "4  **The Role of NVIDIA P100 GPUs in Transformer ...   \n",
       "\n",
       "                                       bm25_contexts  \\\n",
       "0  [∗Equal contribution. Listing order is random....   \n",
       "1  [Table 4: The Transformer generalizes well to ...   \n",
       "2  [n\\nimprovement in our setting. Hence, we excl...   \n",
       "3  [1 Introduction\\nRecurrent neural networks, lo...   \n",
       "4  [output values. These are concatenated and onc...   \n",
       "\n",
       "                                   ensemble_contexts  \\\n",
       "0  [∗Equal contribution. Listing order is random....   \n",
       "1  [Table 4: The Transformer generalizes well to ...   \n",
       "2  [d\\nlanguage modeling tasks [34]. To the best ...   \n",
       "3  [1 Introduction\\nRecurrent neural networks, lo...   \n",
       "4  [. The\\ntraining objective is to reconstruct ¯...   \n",
       "\n",
       "                              hyde_ensemble_contexts  \\\n",
       "0  [∗Equal contribution. Listing order is random....   \n",
       "1  [,\\nbigger models are better, and dropout is v...   \n",
       "2  [Provided proper attribution is provided, Goog...   \n",
       "3  [Recurrent models typically factor computation...   \n",
       "4  [Recurrent models typically factor computation...   \n",
       "\n",
       "                             ensemble_cross_contexts  \\\n",
       "0  [∗Equal contribution. Listing order is random....   \n",
       "1  [s\\nentirely. Experiments on two machine trans...   \n",
       "2  [d\\nlanguage modeling tasks [34]. To the best ...   \n",
       "3  [1 Introduction\\nRecurrent neural networks, lo...   \n",
       "4  [e\\nprevious state-of-the-art model. The Trans...   \n",
       "\n",
       "                                  total_mix_contexts  \n",
       "0  [Provided proper attribution is provided, Goog...  \n",
       "1  [Table 4: The Transformer generalizes well to ...  \n",
       "2  [Provided proper attribution is provided, Goog...  \n",
       "3  [1 Introduction\\nRecurrent neural networks, lo...  \n",
       "4  [In addition, we apply dropout to the sums of ...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['total_mix_contexts'] = test_data.hyde_query.apply(\n",
    "    lambda query: [doc.page_content for doc in cross_encoder_reranker_retriever.invoke(query)]\n",
    ")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1f067d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "test_data.to_excel('data/retrieval_test.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83cb681",
   "metadata": {},
   "source": [
    "### 3\\) Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8bc9efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "컨텍스트 문서: 21개 문서\n",
      "========================================================================================================================================================================================================\n",
      "[Document(metadata={}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works. Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolution')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# 평가셋 Document 객체 리스트로 변환\n",
    "context_docs = test_data.reference_contexts.apply(\n",
    "    lambda x: [Document(page_content=doc) for doc in eval(x)]\n",
    ").tolist()\n",
    "print(f\"컨텍스트 문서: {len(context_docs)}개 문서\")\n",
    "print(\"=\"*200)\n",
    "print(context_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "190d3683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from krag.evaluators import RougeOfflineRetrievalEvaluators\n",
    "\n",
    "def evaluate_qa_test(df_qa_test: pd.Series, max_k=2) -> dict:\n",
    "    # 문서 Document 객체 리스트로 변환\n",
    "    qa_test = df_qa_test.apply(\n",
    "        lambda x: [Document(page_content=doc) for doc in x]\n",
    "    ).tolist()\n",
    "\n",
    "    # 평가자 인스턴스 생성\n",
    "    evaluator = RougeOfflineRetrievalEvaluators(\n",
    "        actual_docs=context_docs,\n",
    "        predicted_docs=qa_test, \n",
    "        match_method='rouge2',\n",
    "        threshold=0.8,\n",
    "    )\n",
    "\n",
    "    # 평가지표 계산\n",
    "    result = []\n",
    "    for k in range(1, max_k+1):\n",
    "        hit_rate = evaluator.calculate_hit_rate(k=k)['hit_rate']\n",
    "        mrr = evaluator.calculate_mrr(k=k)['mrr']\n",
    "        map_score = evaluator.calculate_map(k=k)['map']\n",
    "        ndcg = evaluator.calculate_ndcg(k=k)['ndcg']\n",
    "\n",
    "        print(f\"K={k}\")\n",
    "        print(\"-\"*200)\n",
    "        print(f\"Hit Rate: {hit_rate:.3f}\")\n",
    "        print(f\"MRR: {mrr:.3f}\")\n",
    "        print(f\"MAP: {map_score:.3f}\")\n",
    "        print(f\"NDCG: {ndcg:.3f}\")\n",
    "        print(\"=\"*200)\n",
    "        print()\n",
    "\n",
    "        result.append({\n",
    "            'hit_rate': hit_rate,\n",
    "            'mrr': mrr,\n",
    "            'map': map_score,\n",
    "            'ndcg': ndcg,  \n",
    "        })\n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "08f080b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=1\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 1.000\n",
      "MAP: 0.200\n",
      "NDCG: 1.000\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "K=2\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 1.000\n",
      "MAP: 0.400\n",
      "NDCG: 1.000\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "K=3\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 1.000\n",
      "MAP: 0.600\n",
      "NDCG: 1.000\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "K=4\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.048\n",
      "MRR: 1.000\n",
      "MAP: 0.800\n",
      "NDCG: 1.000\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "K=5\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 1.000\n",
      "MRR: 1.000\n",
      "MAP: 1.000\n",
      "NDCG: 1.000\n",
      "========================================================================================================================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_qa_test(test_data.retrieved_contexts, max_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "19f25464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=1\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 0.286\n",
      "MAP: 0.057\n",
      "NDCG: 1.000\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "K=2\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 0.333\n",
      "MAP: 0.076\n",
      "NDCG: 0.908\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "K=3\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 0.365\n",
      "MAP: 0.105\n",
      "NDCG: 0.834\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "K=4\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 0.389\n",
      "MAP: 0.119\n",
      "NDCG: 0.805\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "K=5\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 0.389\n",
      "MAP: 0.119\n",
      "NDCG: 0.805\n",
      "========================================================================================================================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_qa_test(test_data.bm25_contexts, max_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "99f1c21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=1\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 1.000\n",
      "MAP: 0.200\n",
      "NDCG: 1.000\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "K=2\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 1.000\n",
      "MAP: 0.400\n",
      "NDCG: 1.000\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "K=3\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 1.000\n",
      "MAP: 0.600\n",
      "NDCG: 1.000\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "K=4\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.143\n",
      "MRR: 1.000\n",
      "MAP: 0.800\n",
      "NDCG: 1.000\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "K=5\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 1.000\n",
      "MRR: 1.000\n",
      "MAP: 0.971\n",
      "NDCG: 1.000\n",
      "========================================================================================================================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_qa_test(test_data.ensemble_contexts, max_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "412f5d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=1\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 0.667\n",
      "MAP: 0.133\n",
      "NDCG: 1.000\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "K=2\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 0.738\n",
      "MAP: 0.205\n",
      "NDCG: 0.924\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "K=3\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 0.754\n",
      "MAP: 0.281\n",
      "NDCG: 0.892\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "K=4\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 0.778\n",
      "MAP: 0.348\n",
      "NDCG: 0.855\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "K=5\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 0.787\n",
      "MAP: 0.370\n",
      "NDCG: 0.850\n",
      "========================================================================================================================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_qa_test(test_data.hyde_ensemble_contexts, max_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a916e795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=1\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 0.857\n",
      "MAP: 0.171\n",
      "NDCG: 1.000\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "K=2\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 0.929\n",
      "MAP: 0.310\n",
      "NDCG: 0.965\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "K=3\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 0.929\n",
      "MAP: 0.446\n",
      "NDCG: 0.961\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "K=4\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 0.929\n",
      "MAP: 0.446\n",
      "NDCG: 0.961\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "K=5\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 0.929\n",
      "MAP: 0.446\n",
      "NDCG: 0.961\n",
      "========================================================================================================================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_qa_test(test_data.ensemble_reranker_contexts, max_k=5) # 가장 우수하다고 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "07ddf7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=1\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 0.714\n",
      "MAP: 0.143\n",
      "NDCG: 1.000\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "K=2\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 0.762\n",
      "MAP: 0.248\n",
      "NDCG: 0.968\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "K=3\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 0.794\n",
      "MAP: 0.305\n",
      "NDCG: 0.901\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "K=4\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 0.794\n",
      "MAP: 0.305\n",
      "NDCG: 0.901\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "K=5\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hit Rate: 0.000\n",
      "MRR: 0.794\n",
      "MAP: 0.305\n",
      "NDCG: 0.901\n",
      "========================================================================================================================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_qa_test(test_data.total_mix_contexts, max_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035bf15c",
   "metadata": {},
   "source": [
    "### 4\\) Generation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "272067a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>hyde_query</th>\n",
       "      <th>bm25_contexts</th>\n",
       "      <th>ensemble_contexts</th>\n",
       "      <th>hyde_ensemble_contexts</th>\n",
       "      <th>total_mix_contexts</th>\n",
       "      <th>ensemble_reranker_contexts</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is Niki Parmar in the context of the Trans...</td>\n",
       "      <td>['Provided proper attribution is provided, Goo...</td>\n",
       "      <td>Niki Parmar is listed as an author of the pape...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "      <td>Niki Parmar is one of the contributors to the ...</td>\n",
       "      <td>Niki Parmar is one of the co-authors of the se...</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "      <td>[Provided proper attribution is provided, Goog...</td>\n",
       "      <td>[∗Equal contribution. Listing order is random....</td>\n",
       "      <td>Niki Parmar is one of the contributors to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How is the Transformer model applied to Englis...</td>\n",
       "      <td>['s\\nentirely. Experiments on two machine tran...</td>\n",
       "      <td>The Transformer model is applied successfully ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[Table 4: The Transformer generalizes well to ...</td>\n",
       "      <td>The Transformer model is applied to English co...</td>\n",
       "      <td>**Application of the Transformer Model to Engl...</td>\n",
       "      <td>[Table 4: The Transformer generalizes well to ...</td>\n",
       "      <td>[Table 4: The Transformer generalizes well to ...</td>\n",
       "      <td>[,\\nbigger models are better, and dropout is v...</td>\n",
       "      <td>[Table 4: The Transformer generalizes well to ...</td>\n",
       "      <td>[s\\nentirely. Experiments on two machine trans...</td>\n",
       "      <td>The Transformer model is applied to English co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Whaat is a Tranformer and whoo were the main p...</td>\n",
       "      <td>['∗Equal contribution. Listing order is random...</td>\n",
       "      <td>The Transformer was first designed and impleme...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[d\\nlanguage modeling tasks [34]. To the best ...</td>\n",
       "      <td>The Transformer is a neural network architectu...</td>\n",
       "      <td>**Transformer: Definition and Key Contributors...</td>\n",
       "      <td>[n\\nimprovement in our setting. Hence, we excl...</td>\n",
       "      <td>[d\\nlanguage modeling tasks [34]. To the best ...</td>\n",
       "      <td>[Provided proper attribution is provided, Goog...</td>\n",
       "      <td>[Provided proper attribution is provided, Goog...</td>\n",
       "      <td>[d\\nlanguage modeling tasks [34]. To the best ...</td>\n",
       "      <td>A Transformer is a neural network architecture...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are gated recurrent neural networks used ...</td>\n",
       "      <td>['1 Introduction\\nRecurrent neural networks, l...</td>\n",
       "      <td>Gated recurrent neural networks have been firm...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[1 Introduction\\nRecurrent neural networks, lo...</td>\n",
       "      <td>Gated recurrent neural networks are used as st...</td>\n",
       "      <td>**Gated Recurrent Neural Networks in Sequence ...</td>\n",
       "      <td>[1 Introduction\\nRecurrent neural networks, lo...</td>\n",
       "      <td>[1 Introduction\\nRecurrent neural networks, lo...</td>\n",
       "      <td>[Recurrent models typically factor computation...</td>\n",
       "      <td>[1 Introduction\\nRecurrent neural networks, lo...</td>\n",
       "      <td>[1 Introduction\\nRecurrent neural networks, lo...</td>\n",
       "      <td>Gated recurrent neural networks are used as st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what p100 gpus do in transformer training? i d...</td>\n",
       "      <td>['Recurrent models typically factor computatio...</td>\n",
       "      <td>The Transformer model can be trained to reach ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>[. The\\ntraining objective is to reconstruct ¯...</td>\n",
       "      <td>Based on the provided context, the role of GPU...</td>\n",
       "      <td>**The Role of NVIDIA P100 GPUs in Transformer ...</td>\n",
       "      <td>[output values. These are concatenated and onc...</td>\n",
       "      <td>[. The\\ntraining objective is to reconstruct ¯...</td>\n",
       "      <td>[Recurrent models typically factor computation...</td>\n",
       "      <td>[In addition, we apply dropout to the sums of ...</td>\n",
       "      <td>[e\\nprevious state-of-the-art model. The Trans...</td>\n",
       "      <td>Based on the context, P100 GPUs are used to pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Who is Niki Parmar in the context of the Trans...   \n",
       "1  How is the Transformer model applied to Englis...   \n",
       "2  Whaat is a Tranformer and whoo were the main p...   \n",
       "3  What are gated recurrent neural networks used ...   \n",
       "4  what p100 gpus do in transformer training? i d...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  ['Provided proper attribution is provided, Goo...   \n",
       "1  ['s\\nentirely. Experiments on two machine tran...   \n",
       "2  ['∗Equal contribution. Listing order is random...   \n",
       "3  ['1 Introduction\\nRecurrent neural networks, l...   \n",
       "4  ['Recurrent models typically factor computatio...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Niki Parmar is listed as an author of the pape...   \n",
       "1  The Transformer model is applied successfully ...   \n",
       "2  The Transformer was first designed and impleme...   \n",
       "3  Gated recurrent neural networks have been firm...   \n",
       "4  The Transformer model can be trained to reach ...   \n",
       "\n",
       "                       synthesizer_name  \\\n",
       "0  single_hop_specifc_query_synthesizer   \n",
       "1  single_hop_specifc_query_synthesizer   \n",
       "2  single_hop_specifc_query_synthesizer   \n",
       "3  single_hop_specifc_query_synthesizer   \n",
       "4  single_hop_specifc_query_synthesizer   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [∗Equal contribution. Listing order is random....   \n",
       "1  [Table 4: The Transformer generalizes well to ...   \n",
       "2  [d\\nlanguage modeling tasks [34]. To the best ...   \n",
       "3  [1 Introduction\\nRecurrent neural networks, lo...   \n",
       "4  [. The\\ntraining objective is to reconstruct ¯...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Niki Parmar is one of the contributors to the ...   \n",
       "1  The Transformer model is applied to English co...   \n",
       "2  The Transformer is a neural network architectu...   \n",
       "3  Gated recurrent neural networks are used as st...   \n",
       "4  Based on the provided context, the role of GPU...   \n",
       "\n",
       "                                          hyde_query  \\\n",
       "0  Niki Parmar is one of the co-authors of the se...   \n",
       "1  **Application of the Transformer Model to Engl...   \n",
       "2  **Transformer: Definition and Key Contributors...   \n",
       "3  **Gated Recurrent Neural Networks in Sequence ...   \n",
       "4  **The Role of NVIDIA P100 GPUs in Transformer ...   \n",
       "\n",
       "                                       bm25_contexts  \\\n",
       "0  [∗Equal contribution. Listing order is random....   \n",
       "1  [Table 4: The Transformer generalizes well to ...   \n",
       "2  [n\\nimprovement in our setting. Hence, we excl...   \n",
       "3  [1 Introduction\\nRecurrent neural networks, lo...   \n",
       "4  [output values. These are concatenated and onc...   \n",
       "\n",
       "                                   ensemble_contexts  \\\n",
       "0  [∗Equal contribution. Listing order is random....   \n",
       "1  [Table 4: The Transformer generalizes well to ...   \n",
       "2  [d\\nlanguage modeling tasks [34]. To the best ...   \n",
       "3  [1 Introduction\\nRecurrent neural networks, lo...   \n",
       "4  [. The\\ntraining objective is to reconstruct ¯...   \n",
       "\n",
       "                              hyde_ensemble_contexts  \\\n",
       "0  [∗Equal contribution. Listing order is random....   \n",
       "1  [,\\nbigger models are better, and dropout is v...   \n",
       "2  [Provided proper attribution is provided, Goog...   \n",
       "3  [Recurrent models typically factor computation...   \n",
       "4  [Recurrent models typically factor computation...   \n",
       "\n",
       "                                  total_mix_contexts  \\\n",
       "0  [Provided proper attribution is provided, Goog...   \n",
       "1  [Table 4: The Transformer generalizes well to ...   \n",
       "2  [Provided proper attribution is provided, Goog...   \n",
       "3  [1 Introduction\\nRecurrent neural networks, lo...   \n",
       "4  [In addition, we apply dropout to the sums of ...   \n",
       "\n",
       "                          ensemble_reranker_contexts  \\\n",
       "0  [∗Equal contribution. Listing order is random....   \n",
       "1  [s\\nentirely. Experiments on two machine trans...   \n",
       "2  [d\\nlanguage modeling tasks [34]. To the best ...   \n",
       "3  [1 Introduction\\nRecurrent neural networks, lo...   \n",
       "4  [e\\nprevious state-of-the-art model. The Trans...   \n",
       "\n",
       "                                              answer  \n",
       "0  Niki Parmar is one of the contributors to the ...  \n",
       "1  The Transformer model is applied to English co...  \n",
       "2  A Transformer is a neural network architecture...  \n",
       "3  Gated recurrent neural networks are used as st...  \n",
       "4  Based on the context, P100 GPUs are used to pr...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG 체인 생성 답변\n",
    "test_data['answer'] = test_data.apply(\n",
    "    lambda row: qa_chain.invoke({\n",
    "        \"context\": \"\\n\".join(row.ensemble_reranker_contexts), \n",
    "        \"query\": row.user_input\n",
    "    }),\n",
    "    axis=1\n",
    ")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a40d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 63/63 [01:12<00:00,  1.16s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 0.5891, 'faithfulness': 0.7242, 'factual_correctness(mode=f1)': 0.3833}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import EvaluationDataset\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness\n",
    "\n",
    "# 평가셋 처리\n",
    "result_data = test_data[['user_input', 'ensemble_reranker_contexts', 'answer', 'reference']]\n",
    "result_data.columns = ['user_input', 'retrieved_contexts', 'response', 'reference']\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_pandas(result_data)\n",
    "\n",
    "# LLM 래퍼 생성\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1\", temperature=0))\n",
    "\n",
    "# 생성 평가\n",
    "result = evaluate(\n",
    "    dataset=evaluation_dataset,   # 평가 데이터셋\n",
    "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness()],   # 평가 메트릭\n",
    "    llm=evaluator_llm,   # LLM 래퍼\n",
    ")\n",
    "result # faithfulness 빼고는 조금 높음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3159c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
